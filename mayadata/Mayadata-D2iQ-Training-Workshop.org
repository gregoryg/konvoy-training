# -*- fill-column: 80; -*-
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:t date:t e:t email:nil f:t inline:t num:nil
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:1
#+options: todo:t |:t
#+title: Mayadata D2iQ Training Workshop
#+date: <2019-08-22 Thu>
#+author: Greg Grubbs, Brian Matheson
#+instructors: Greg Grubbs, Brian Matheson
#+slide_header: github.com/mesosphere/konvoy-training
#+email: ggrubbs@d2iq.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.2 (Org mode 9.2.3)
# #+SETUPFILE: ~/projects/org-html-themes/setup/theme-readtheorg.setup
# #+SETUPFILE: https://raw.githubusercontent.com/fniessen/org-html-themes/master/setup/theme-readtheorg.setup

* Introduction
  :PROPERTIES:
  :CUSTOM_ID: introduction
  :UNNUMBERED: t
  :END:

During this training, you'll learn how to deploy Konvoy and to use its
main features, with OpenEBS as the persistent storage.

* Prerequisites
  :PROPERTIES:
  :CUSTOM_ID: prerequisites
  :UNNUMBERED: t
  :END:

** Workshop syllabus
   Please open the page at [[https://github.com/gregoryg/konvoy-training/blob/master/mayadata/Mayadata-D2iQ-Training-Workshop.org]]


** Workstation or Laptop requirements
You need either a Linux, MacOS or a Windows laptop.

  You may choose to use the
  [[https://console.cloud.google.com/cloudshell][Google Cloud Shell]].

** Jump Servers

Jumpservers have been deployed for each lab participant with all prerequisites
installed. First, go to the participant data spreadsheet and select a host by
entering your name.  Then, download the ssh-private-key (id_rsa_student#) and
change the file permissions.  Finally, ssh to the ipaddress of your assigned
jumpserver using the -i option to specify the identity file to be used.  The
username for the Jumpserver is "centos".

Once you download your SSH key, change the permission on the key
#+begin_src bash
chmod 400 id_rsa_student#
#+end_src

Test your SSH key and access to your jump server
#+begin_src bash
ssh -i id_rsa_student# centos@jumpserver-ip-address
#+end_src

For Windows, you can use the [Google Cloud Shell](https://console.cloud.google.com/cloudshell).
Once your Google Cloud Shell has started, you will have to copy the contents of you id_rsa_student#.pem file to a local file in the cloud shell.  Then change the permission on the file and ssh into the jump host.

Windows users familiar with =putty= may import the =id_rsa_student#.ppk= key in
the Google Drive


* 1. Deploy a Konvoy Kubernetes cluster without Storage Options
  :PROPERTIES:
  :CUSTOM_ID: deploy-a-konvoy-cluster
  :END:
  
** Objectives
   :PROPERTIES:
   :CUSTOM_ID: objectives
   :END:

- Deploy a Kubernetes cluster with all the addons you need to get a production
  ready container orchestration platform
- Configure kubectl to manage your cluster

** Why is this Important?
   :PROPERTIES:
   :CUSTOM_ID: why-is-this-important
   :END:

There are many ways to deploy a kubernetes cluster from a fully manual procedure
to using a fully automated or opinionated SaaS. Cluster sizes can also widely
vary from a single node deployment on your laptop, to thousands of nodes in a
single logical cluster, or even across multiple clusters. Thus, picking a
deployment model that suits the scale that you need as your business grows is
important.

** Lab Steps

  *IMPORTANT*
First change directory into the lab directory:

#+begin_src sh
cd ~/lab
#+end_src

Deploy your cluster using the command: `bin/00-konvoy.sh`

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/00-konvoy.sh
cp -v cluster-STORAGE-ADDONS-DISABLED.yaml cluster.yaml
echo "Now bringing up a Konvoy cluster with storage addons disabled using 'konvoy up --yes'"
konvoy up --yes && konvoy apply kubeconfig --force-overwrite
#+end_src
The output should be similar to:

#+begin_example
  konvoy up --yes                                                                  
  This process will take about 15 minutes to complete (additional time may be required for larger clusters), do you want to continue [y/n]: y

  STAGE [Provisioning Infrastructure]

  Initializing provider plugins...

  ...

  Terraform has been successfully initialized!

  ...

  STAGE [Deploying Enabled Addons]
  helm                                                                   [OK]
  dashboard                                                              [OK]
  fluentbit                                                              [OK]
  traefik                                                                [OK]
  opsportal                                                              [OK]
  kommander                                                              [OK]
  traefik-forward-auth                                                   [OK]
  dex-k8s-authenticator                                                  [OK]

  STAGE [Removing Disabled Addons]

  Kubernetes cluster and addons deployed successfully!

  Run `./konvoy apply kubeconfig` to update kubectl credentials.

  Navigate to the URL below to access various services running in the cluster.
    https://a7e039f1a05a54f45b36e063f5aee077-287582892.us-west-2.elb.amazonaws.com/ops/landing
  And login using the credentials below.
    Username: goofy_einstein
    Password: tUeARRKxM8PfrIy2cjFc1jI0Hr2I0duzlttr1LzRTKoDooQJ0d1yyutjNv4NLHvy

  If the cluster was recently created, the dashboard and services may take a few minutes to be accessible.
#+end_example

If you get any error during the deployment of the addons (it can happen
with network connectivity issues), then, you can run the following
command to redeploy them:

#+begin_src sh
  konvoy deploy addons --yes
#+end_src

As soon as your cluster is successfully deployed, the URL and the credentials to
access your cluster are displayed.  When you lauch your dashboard URL in your
browser the first screen will ask you to select "login or generate token",
select login and use the credentials provided.

If you need to get this information later, you can execute the command
below:

#+begin_src sh
  konvoy get ops-portal
#+end_src

#+CAPTION: Konvoy UI
[[file:images/konvoy-ui.png]]

Click on the =Kubernetes Dashboard= icon to open it.

#+CAPTION: Kubernetes Dashboard
[[file:images/kubernetes-dashboard.png]]

You can check that the Kubernetes cluster has been deployed with 3 control nodes
and 5 worker nodes

#+begin_src sh
  kubectl get nodes
#+end_src
#+begin_example
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-128-64.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-129-247.us-west-2.compute.internal   Ready    <none>   10m   v1.15.2
ip-10-0-129-41.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-129-88.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-130-84.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-193-118.us-west-2.compute.internal   Ready    master   11m   v1.15.2
ip-10-0-193-232.us-west-2.compute.internal   Ready    master   12m   v1.15.2
ip-10-0-194-21.us-west-2.compute.internal    Ready    master   13m   v1.15.2
#+end_example

* 2. Install OpenEBS and make it the default storage for the cluster
** Install storage provider (=iscsi=)
We first install =iscsi= on the worker nodes of our cluster.  This will provide
the base level persistent storage we will use in OpenEBS

To use the script, run =bin/01-install-iscsi.sh=

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/set-env.sh
export CLUSTER=$(cd state; jq -r '.modules[].outputs.cluster_name.value' terraform.tfstate)
# CLUSTER=$(cd state; terraform output cluster_name) # name of your cluster, its the prefix used for worker nodes, check in ec2 console
export REGION=us-west-2
export KEY_FILE=$(realpath *.pem | head -1) # path to private key file in folder where you ran konvoy -up
export DISK_SIZE=150 # 161061273600 bytes
#+end_src

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/01-install-iscsi.sh
. bin/set-env.sh
# IPS=$(aws --region=$REGION ec2 describe-instances |  jq --raw-output ".Reservations[].Instances[] | select((.Tags | length) > 0) | select(.Tags[].Value | test(\"$CLUSTER-worker\")) | select(.State.Name | test(\"running\")) | [.PublicIpAddress] | join(\" \")")
IPS=$(aws --region=$REGION ec2 describe-instances --filters Name=tag:konvoy/nodeRoles,Values=worker Name=tag:konvoy/clusterName,Values=${CLUSTER} | jq -r '.Reservations[].Instances[].PublicIpAddress')
for ip in $IPS; do
	echo $ip
	ssh -o StrictHostKeyChecking=no -i $KEY_FILE centos@$ip sudo yum install iscsi-initiator-utils -y
	ssh -i $KEY_FILE centos@$ip sudo systemctl enable iscsid
	ssh -i $KEY_FILE centos@$ip sudo systemctl start iscsid
done
# show that iscsi is running
for ip in $IPS; do
	echo $ip
	ssh -i $KEY_FILE centos@$ip cat /etc/iscsi/initiatorname.iscsi
done

#+end_src

#+RESULTS:

** Attach additional disk to worker nodes for OpenEBS
We next create new raw disk volumes on AWS on our worker nodes, and attach those
raw volumes so they will be available to OpenEBS

Run the script = bin/02-create-and-attach-disk.sh=

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/02-create-and-attach-disk.sh
. bin/set-env.sh

# Get all running instances of worker nodes
aws --region=$REGION ec2 describe-instances --filters Name=tag:konvoy/nodeRoles,Values=worker Name=tag:konvoy/clusterName,Values=${CLUSTER} | jq -r '.Reservations[].Instances[] | select(.State.Name | test("running")) | [.InstanceId, .Placement.AvailabilityZone] | "\(.[0]) \(.[1])"' | \
	while read instance zone; do
		echo $instance $zone
		# For each running worker instance, create a volume then attach that new volume as /dev/xvdg
		volume=$(aws --region=$REGION ec2 create-volume --size=$DISK_SIZE --volume-type gp2 --availability-zone=$zone --tag-specifications="ResourceType=volume,Tags=[{Key=string,Value=$CLUSTER}, {Key=owner,Value=michaelbeisiegel}]" | jq --raw-output .VolumeId)
		sleep 10
		aws --region=$REGION ec2 attach-volume --device=/dev/xvdg --instance-id=$instance --volume-id=$volume
	done
#+end_src

** Install the storage provider operator

From here on, we will use standard Kubernetes commands and UI. 

This step installs OpenEBS on our Konvoy cluster using the =kubectl= command.

Run the script =bin/03-install-openebs-operator.sh=

We will use the manifest for latest version (1.3) in the workshop
#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/03-install-openebs-operator.sh
kubectl apply -f openebs-operator.yaml
#+end_src

*** Not included in our lab                                        :noexport:
Note: The devices that the worker nodes have attached from the Konvoy install,
here /dev/nvme0n1,/dev/nvme1n1, maybe different in your case. You can find out
the device names by ssh’ing into one of the worker nodes and using the =lsblk=
command.

We will use a pre-edited operator yaml file that excludes the disk devices used
in our AWS instances
#+begin_src sh :session konvoy-sh :results none
# sed 's!exclude: \(.\+/dev/md\)!exclude: \1,/dev/nvme0n1,/dev/nvme1n1,/dev/nvme2n1,/dev/nvme3n1,/dev/nvme4n1,/dev/nvme5n1,/dev/nvme6n1!' ./openebs-operator-1.0.0.yaml > EDITED-openebs-operator-1.0.0.yaml
# kubectl apply -f EDITED-openebs-operator-1.0.0.yaml
#+end_src

** Create OpenEBS cstor storage pools
   Now we will create the =StoragePoolClaim= resource

   Pick only the 150GB devices we created above (selected by size)

   Run the script =bin/04-create-storage-pools.sh=

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/04-create-storage-pools.sh
blkdevices=$(kubectl get blockdevices -n openebs|egrep '^blockdevice' |grep 161061273600 | cut -d' ' -f1)

cat <<EOF | kubectl apply -f -
kind: StoragePoolClaim
apiVersion: openebs.io/v1alpha1
metadata:
  name: cstor
  annotations:
    cas.openebs.io/config: |
      - name: PoolResourceRequests
        value: |-
          memory: 2Gi
      - name: PoolResourceLimits
        value: |-
          memory: 4Gi
spec:
  name: cstor
  type: disk
  poolSpec:
    poolType: striped
  blockDevices:
    blockDeviceList:
$(echo $blkdevices | sed 's,\([a-z0-9-]\+\),\n      - \1,g')
EOF
#+end_src

** Create a default storage class

In this step, we create a storage class named =openebs-cstor-default=. This
storage class will use the storage pool created out of blockdevices we have
specified with an additional annotation that makes it the default storage class
for the konvoy cluster.

Run the script =bin/05-create-default-storage-class.sh=

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/05-create-default-storage-class.sh
cat <<EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: openebs-cstor-default
  annotations:
    openebs.io/cas-type: cstor    
    cas.openebs.io/config: |
      - name: StoragePoolClaim
        value: "cstor"
      - name: ReplicaCount
        value: "3"
    openebs.io/cas-type: cstor
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: openebs.io/provisioner-iscsi
EOF
#+end_src

** Verify storage
   Finally, before we enable the storage-dependent add-ons for Konvoy, let's
   verify that our OpenEBS installation is working.  

   We define and start a simple Pod that requests a Persistent Volume (PV).
   This will use our =cstor= default storage provisioner in OpenEBS.  After the
   pod has initialized and started running, we should see our simple time stamp
   output.  

   Run =bin/06-verify-storage.sh= and wait for the data output to appear.

   Type =Ctl-C= to interrupt and get back to the shell after viewing the output.

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/06-verify-storage.sh
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-test
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: openebs-cstor-default
  resources:
    requests:
      storage: 1Gi
---
kind: Pod
apiVersion: v1
metadata:
  name: pod-pv-test
spec:
  volumes:
    - name: pv-test
      persistentVolumeClaim:
        claimName: pvc-test
  containers:
   - name: test
     image: centos
     command: ["/bin/sh"]
     args: ["-c", "while true; do echo \">>> \"$(date) >> /data/output; sleep 10; done"]
     volumeMounts:
       - mountPath: "/data"
         name: pv-test
EOF

echo "Waiting for pod to start running..."
for i in {0..10}
do
	podphase=$(kubectl get pod pod-pv-test -o json | jq -r '.status.phase')
	if [ "$podphase" != "Running" ] ; then
		echo -n .
		sleep 5
	else
		echo
		break
	fi
done

echo "tailing output from the container using OpenEBS - type ctl-c when done"

kubectl exec pod-pv-test -it --  tail -f /data/output
#+end_src

** Re-enable all storage add-ons

And now we enable all the storage-dependent Konvoy addons.  These components
will now use our default OpenEBS storage.

Run the script =bin/07-re-enable-addons.sh=

#+begin_src sh :session konvoy-sh :shebang "#!/usr/bin/env bash" :results none :tangle bin/07-re-enable-addons.sh
cp -v cluster-STORAGE-ADDONS-ENABLED.yaml cluster.yaml
echo "Now enabling all addons with 'konvoy deploy addons --yes'"
konvoy deploy addons --yes
#+end_src

* 3. Expose a Kubernetes Application using a Service Type Load Balancer (L4)
   :PROPERTIES:
   :CUSTOM_ID: expose-a-kubernetes-application-using-a-service-type-load-balancer-l4
   :END:

** Objectives
   :PROPERTIES:
   :CUSTOM_ID: objectives-1
   :END:

- Deploy a Redis pod and expose it using a Service Type Load Balancer
  (L4) and validate that the connection is exposed to the outside
- Deploy a couple hello-world applications and expose them using an
  Ingress service (L7) and validate that the connection is exposed to
  the outside
  [[https://www.webopedia.com/quick_ref/OSI_Layers.asp][The 7 Layers of the OSI Model]]
** Why is this Important?
   :PROPERTIES:
   :CUSTOM_ID: why-is-this-important-1
   :END:

Exposing your application on a kubernetes cluster in an Enterprise-grade
environment can be challenging to set up. With Konvoy, the integration
with AWS cloud load balancer is already done by default and Traefik is
deployed to allow you to easily create ingresses.

** Lab Steps
Deploy a Redis pod on your Kubernetes cluster by running the following
command:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: redis
  name: redis
spec:
  containers:
  - name: redis
    image: redis:5.0.3
    ports:
    - name: redis
      containerPort: 6379
      protocol: TCP
EOF
#+end_src

Then, expose the service, you need to run the following command to
create a Service Type Load Balancer:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Service
metadata:
  labels:
    app: redis
  name: redis
spec:
  type: LoadBalancer
  selector:
    app: redis
  ports:
  - protocol: TCP
    port: 6379
    targetPort: 6379
EOF
#+end_src

Finally, run the following command to see the URL of the Load Balancer
created on AWS for this service:

#+begin_src sh
kubectl get svc redis
#+end_src
#+begin_example
  NAME    TYPE           CLUSTER-IP   EXTERNAL-IP                                                               PORT(S)          AGE
  Redis   LoadBalancer   10.0.51.32   a92b6c9216ccc11e982140acb7ee21b7-1453813785.us-west-2.elb.amazonaws.com   6379:31423/TCP   43s
#+end_example

You need to wait for a few minutes while the Load Balancer is created on
AWS and the name resolution in place.

#+begin_src sh
until nslookup $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname})
do
    sleep 1
done
#+end_src

Expected output:
#+begin_example
  ,** server can't find aa4b038c75236642febfeadf2a1e9e304-1736643327.us-west-2.elb.amazonaws.com: NXDOMAIN

  Server:         169.254.169.254
  Address:        169.254.169.254#53
  ### (above lines repeated)
  Server:         169.254.169.254
  Address:        169.254.169.254#53

  Non-authoritative answer:
  Name:   a4b038c75236642febfeadf2a1e9e304-1736643327.us-west-2.elb.amazonaws.com
  Address: 52.34.37.52
  Name:   a4b038c75236642febfeadf2a1e9e304-1736643327.us-west-2.elb.amazonaws.com
  Address: 54.148.3.99
#+end_example

You can validate that you can access the Redis pod from your laptop
using telnet:

#+begin_src sh
telnet $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src
#+begin_example
  Trying 52.27.218.48...
  Connected to a92b6c9216ccc11e982140acb7ee21b7-1453813785.us-west-2.elb.amazonaws.com.
  Escape character is '^]'.
  quit
  +OK
  Connection closed by foreign host.
#+end_example

NOTE: To exit =telnet=, type =Control-]=, then =quit=

If you don't have =telnet= installed in your machine, you can use =nc=
instead:

#+begin_src sh
sudo apt install netcat
nc -z $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379 < /dev/null ; echo $?
#+end_src
* 4. Expose a Kubernetes Application using an Ingress (L7)
  :PROPERTIES:
  :CUSTOM_ID: expose-a-kubernetes-application-using-an-ingress-l7
  :END:

Deploy 2 web application pods on your Kubernetes cluster running the
following command:

#+begin_src sh
  kubectl run --restart=Never --image hashicorp/http-echo --labels app=http-echo-1 --port 80 http-echo-1 -- -listen=:80 --text="Hello from http-echo-1"
  kubectl run --restart=Never --image hashicorp/http-echo --labels app=http-echo-2 --port 80 http-echo-2 -- -listen=:80 --text="Hello from http-echo-2"
#+end_src

Then, expose the pods with a Service Type NodePort using the following
commands:

#+begin_src sh
  kubectl expose pod http-echo-1 --port 80 --target-port 80 --type NodePort --name "http-echo-1"
  kubectl expose pod http-echo-2 --port 80 --target-port 80 --type NodePort --name "http-echo-2"
#+end_src

Create the Ingress to expose the application to the outside
world using the following command:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: echo
spec:
  rules:
  - host: "http-echo-1.com"
    http:
      paths:
      - backend:
          serviceName: http-echo-1
          servicePort: 80
  - host: "http-echo-2.com"
    http:
      paths:
      - backend:
          serviceName: http-echo-2
          servicePort: 80
EOF
#+end_src

Go to the Traefik UI to check that new frontends have been created.

#+CAPTION: Traefik front ends
[[file:images/traefik-frontends.png]]

Finally, run the following command to see the URL of the Load Balancer
created on AWS for the Traefik service:

#+begin_src sh
kubectl get svc traefik-kubeaddons -n kubeaddons
#+end_src
#+BEGIN_EXAMPLE
  NAME                 TYPE           CLUSTER-IP    EXTERNAL-IP                                                             PORT(S)                                     AGE
  traefik-kubeaddons   LoadBalancer   10.0.24.215   abf2e5bda6ca811e982140acb7ee21b7-37522315.us-west-2.elb.amazonaws.com   80:31169/TCP,443:32297/TCP,8080:31923/TCP   4h22m
#+END_EXAMPLE


You can validate that you can access the web application pods from your laptop using the following commands:

#+begin_src sh
curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

You can also set some Traefik annotations to use some advanced features
as described in this
[[https://docs.traefik.io/configuration/backends/kubernetes/][document]].

* 5. Leverage Network Policies to restrict access
  :PROPERTIES:
  :CUSTOM_ID: leverage-network-policies-to-restrict-access
  :END:

By default, all the pods can access all the services inside and outside
the Kubernetes clusters and services exposed to the external world can
be accessed by anyone. Kubernetes Network Policies can be used to
restrict access.

When a Kubernetes cluster is deployed by Konvoy, a Calico cluster is
automatically deployed on this cluster. It allows a user to define
network policies without any additional configuration.

** Objectives
   :PROPERTIES:
   :CUSTOM_ID: objectives-2
   :END:

- Create a network policy to deny any ingress
- Check that the Redis and the http-echo apps aren't accessible anymore
- Create network policies to allow ingress access to these apps only
- Check that the Redis and the http-echo apps are now accessible

** Why is this Important?
   :PROPERTIES:
   :CUSTOM_ID: why-is-this-important-2
   :END:

In many cases, you want to restrict communications between services. For
example, you often want some micro services to be reachable only other specific
micro services.

In this lab, we restrict access to ingresses, so you may think that it's useless
as we can simply not expose these apps if we want to restrict access. But, in
fact, it makes sense to also create network policies to avoid cases where an app
is exposed by mistake.

** COMMENT Lab Steps
Create a network policy to deny any ingress

#+begin_src sh :session konvoy-sh
cat <<EOF | kubectl create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF
#+end_src

#+RESULTS:

Wait for a minute to allow the network policy to be activated and check
that the Redis and the http-echo apps aren't accessible anymore

#+begin_src sh
telnet $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src

#+begin_src sh
curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

Create network policies to allow ingress access to these apps only

#+begin_src sh
cat <<EOF | kubectl create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-redis
spec:
  podSelector:
    matchLabels:
      app: redis
  ingress:
  - from: []
EOF

cat <<EOF | kubectl create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-http-echo-1
spec:
  podSelector:
    matchLabels:
      app: http-echo-1
  ingress:
  - from: []
EOF

cat <<EOF | kubectl create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-http-echo-2
spec:
  podSelector:
    matchLabels:
      app: http-echo-2
  ingress:
  - from: []
EOF
#+end_src

Wait for a minute and check that the Redis and the http-echo apps are
now accessible

#+begin_src sh
telnet $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src

#+RESULTS:

#+begin_src sh
  curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
  curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

Delete the network policy that denies any ingress

#+begin_src sh
cat <<EOF | kubectl delete -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF
#+end_src

* 6. Deploy Jenkins using Helm
  :PROPERTIES:
  :CUSTOM_ID: deploy-jenkins-using-helm
  :END:

Helm is a tool for managing Kubernetes charts. Charts are packages of
pre-configured Kubernetes resources.

You can find many charts on the [[https://hub.helm.sh/][Helm Hub]].

** Lab Steps
In this lab, we'll deploy the
[[https://hub.helm.sh/charts/stable/jenkins][Jenkins Helm chart]].

To deploy the chart, you need to run the following command:

#+begin_src sh 
  helm install stable/jenkins --name jenkins --version 1.5.0 --set master.adminPassword=password
#+end_src

#+RESULTS:
#+begin_example
  NAME:   jenkins
  LAST DEPLOYED: Wed Aug  7 17:21:32 2019
  NAMESPACE: default
  STATUS: DEPLOYED

  RESOURCES:
  ==> v1/ConfigMap
  NAME           DATA  AGE
  jenkins        5     1s
  jenkins-tests  1     1s

  ==> v1/Deployment
  NAME     READY  UP-TO-DATE  AVAILABLE  AGE
  jenkins  0/1    1           0          1s

  ==> v1/PersistentVolumeClaim
  NAME     STATUS   VOLUME                CAPACITY  ACCESS MODES  STORAGECLASS  AGE
  jenkins  Pending  awsebscsiprovisioner  1s

  ==> v1/Pod(related)
  NAME                     READY  STATUS   RESTARTS  AGE
  jenkins-c79f457cb-ccttb  0/1    Pending  0         1s

  ==> v1/Role
  NAME                     AGE
  jenkins-schedule-agents  1s

  ==> v1/RoleBinding
  NAME                     AGE
  jenkins-schedule-agents  1s

  ==> v1/Secret
  NAME     TYPE    DATA  AGE
  jenkins  Opaque  2     1s

  ==> v1/Service
  NAME           TYPE          CLUSTER-IP  EXTERNAL-IP  PORT(S)         AGE
  jenkins        LoadBalancer  10.0.9.26   <pending>    8080:30323/TCP  1s
  jenkins-agent  ClusterIP     10.0.41.64  <none>       50000/TCP       1s

  ==> v1/ServiceAccount
  NAME     SECRETS  AGE
  jenkins  1        1s
#+end_example


Then, run the following command to get the URL of the Load Balancer
created on AWS for this service:

#+begin_src sh
  kubectl get svc jenkins
#+end_src

#+begin_example
NAME      TYPE           CLUSTER-IP   EXTERNAL-IP                                                              PORT(S)          AGE
  jenkins   LoadBalancer   10.0.9.26    a71b8025991124a90b2babf7ba2a75da-492974167.us-west-2.elb.amazonaws.com   8080:30323/TCP   16m
#+end_example

You need to wait for a few minutes while the Load Balancer is created on
AWS and the name resolution in place.

#+begin_src sh :results replace verbatim :exports code
until nslookup $(kubectl get svc jenkins --output jsonpath={.status.loadBalancer.ingress[*].hostname})
do
    sleep 1
done
echo "Open http://$(kubectl get svc jenkins --output jsonpath={.status.loadBalancer.ingress[*].hostname}):8080 to access the Jenkins UI"
#+end_src

#+begin_example
Server:		100.115.92.193
Address:	100.115.92.193#53

Non-authoritative answer:
Name:	a3b2b652ab4354e7f8eded17d4323afb-1148896961.us-west-2.elb.amazonaws.com
Address: 52.37.201.113
Name:	a3b2b652ab4354e7f8eded17d4323afb-1148896961.us-west-2.elb.amazonaws.com
Address: 52.34.15.160

Open http://a3b2b652ab4354e7f8eded17d4323afb-1148896961.us-west-2.elb.amazonaws.com:8080 to access the Jenkins UI
#+end_example


Go to the corresponding URL to access Jenkins

Login with the user =admin= and the password =password=.

* 7. Deploy Apache Kafka using KUDO
  :PROPERTIES:
  :CUSTOM_ID: deploy-apache-kafka-using-kudo
  :END:

** Objectives

- Deploy a stateful application that requires orchestration to deploy
  (ZooKeeper, then Kafka)
- Show the usefulness of a purely declarative means of controlling software
  lifecycle using specification, not Go code


** Why is this Important?

The Kubernetes Universal Declarative Operator (KUDO) is a highly productive
toolkit for writing operators for Kubernetes. Using KUDO, you can deploy your
applications, give your users the tools they need to operate it and manage its
lifecyle, and understand how it's behaving in their environments.  

Orchestration is handled purely by declaration in YAML without requiring code
for conditional logic and other orchestration requirements.


** Resources
   Kudo community web site at kudo.dev
   
   Public code hosted at https://github.com/kudobuilder

   Community Slack channel at kubernetes.slack.com #kudo


** Lab Steps
*** Download or clone the KUDO Operators we will use               :noexport:
We will need to have the KUDO operators in order to install ZooKeeper and
Kafka.  

If you use =git=, simply clone the KUDO operators repository
#+begin_src 
git clone https://github.com/kudobuilder/operators.git
#+end_src

Otherwise, download a ZIP file of the directory by visiting
https://github.com/kudobuilder/operators, then clicking on the "Clone or
download" button and selecting "Download ZIP"

*** Install KUDO on the Konvoy cluster

Run the following command to deploy KUDO on your Kubernetes cluster:

#+begin_src sh
kubectl kudo init
#+end_src

Check the status of the KUDO controller:

#+begin_src sh
  kubectl get pods -n kudo-system
#+end_src

#+begin_example
  NAME                        READY   STATUS    RESTARTS   AGE
  kudo-controller-manager-0   1/1     Running   0          84s
#+end_example

*** Install the KUDO CLI                                           :noexport:
    KUDO requires downloading a =kubectl-kudo= command for the command line interface.  
**** MacOS

#+begin_src sh
  brew tap kudobuilder/tap
  brew install kudo-cli
#+end_src
**** Linux
#+begin_src sh
  wget https://github.com/kudobuilder/kudo/releases/download/v0.8.0/kubectl-kudo_0.8.0_linux_x86_64
  sudo mv kubectl-kudo_0.8.0_linux_x86_64 /usr/local/bin/kubectl-kudo
  chmod +x /usr/bin/kubectl-kudo
#+end_src

**** Verify installation
     #+begin_src sh
     kubectl-kudo version
     kubectl kudo version
     #+end_src

     Expected output from both commands
     #+begin_example
     KUDO Version: version.Info{GitVersion:"0.7.2", GitCommit:"f9f6aa85", BuildDate:"2019-08-02T15:05:56Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"}
     #+end_example



*** Deploy ZooKeeper using KUDO

TODO: grab current output of install for example block

We will need the =operators= directory that you downloaded at the beginning of this lab.

#+begin_src sh
kubectl kudo install zookeeper --instance=zk
#+end_src
#+begin_example
  operator.kudo.k8s.io/v1alpha1/zookeeper created
  operatorversion.kudo.k8s.io/v1alpha1/zookeeper-0.1.0 created
  No instance named 'zk' tied to this 'zookeeper' version has been found. Do you want to create one? (Yes/no) yes
  instance.kudo.k8s.io/v1alpha1/zk created
#+end_example

Check the status of the deployment:

#+begin_src sh
  kubectl kudo plan status --instance=zk
#+end_src
#+begin_example
  Plan(s) for "zk" in namespace "default":
  .
  └── zk (Operator-Version: "zookeeper-0.1.0" Active-Plan: "zk-deploy-694218097")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase zookeeper (parallel strategy) [COMPLETE]
      │       └── Step everything (COMPLETE)
      └── Plan validation (serial strategy) [NOT ACTIVE]
          └── Phase connection (parallel strategy) [NOT ACTIVE]
              └── Step connection (parallel strategy) [NOT ACTIVE]
                  └── connection [NOT ACTIVE]
#+end_example

And check that the corresponding pods are running:

#+begin_src sh
  kubectl get pods | grep zk
#+end_src
#+begin_example
  zk-zookeeper-0                    1/1     Running   0          81s
  zk-zookeeper-1                    1/1     Running   0          81s
  zk-zookeeper-2                    1/1     Running   0          81s
#+end_example

*** Deploy Kafka using KUDO
Deploy Kafka 2.2.1 using KUDO (the version of the KUDO Kafka operator is 0.1.3):

#+begin_src sh
kubectl kudo install kafka --instance=kafka -p ZOOKEEPER_URI=zk-zookeeper-0.zk-hs:2181,zk-zookeeper-1.zk-hs:2181,zk-zookeeper-2.zk-hs:2181 --version=0.1.3
#+end_src

Check the status of the deployment.  This will only show status when deploying, otherwise will report a plan status is not found.

#+begin_src sh
  kubectl kudo plan status --instance=kafka
#+end_src
#+begin_example
  Plan(s) for "kafka" in namespace "default":
  .
  └── kafka (Operator-Version: "kafka-0.1.2" Active-Plan: "kafka-deploy-975266742")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase deploy-kafka (serial strategy) [COMPLETE]
      │       └── Step deploy (COMPLETE)
      └── Plan not-allowed (serial strategy) [NOT ACTIVE]
          └── Phase not-allowed (serial strategy) [NOT ACTIVE]
              └── Step not-allowed (serial strategy) [NOT ACTIVE]
                  └── not-allowed [NOT ACTIVE]
#+end_example

And check that the corresponding pods are running:

#+begin_src sh
  kubectl get pods | grep kafka
#+end_src

#+begin_example
  kafka-kafka-0                          1/1     Running   0          39s
  kafka-kafka-1                          1/1     Running   0          58s
  kafka-kafka-2                          1/1     Running   0          118s
#+end_example

*** Produce messages in Kafka
    This Deployment will continuously place messages on a Kafka topic

#+begin_src sh :session konvoy-sh :results none
cat <<EOF | kubectl create -f -
  apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    name: kudo-kafka-generator
  spec:
    replicas: 1
    template:
      metadata:
        name: kudo-kafka-generator
        labels:
          app: kudo-kafka-generator
      spec:
        containers:
        - name: kudo-kafka-generator
          image: mesosphere/flink-generator:0.1
          command: ["/generator-linux"]
          imagePullPolicy: Always
          args: ["--broker", "kafka-kafka-0.kafka-svc:9092"]
EOF
#+end_src


*** Consume messages from Kafka:

#+begin_src sh :session konvoy-sh :results none
cat <<EOF | kubectl create -f -
  apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
   name: kudo-kafka-consumer
  spec:
   replicas: 1
   template:
     metadata:
       name: kudo-kafka-consumer
       labels:
         app: kudo-kafka-consumer
     spec:
       containers:
       - name: kudo-kafka-consumer
         image: tbaums/kudo-kafka-demo
         imagePullPolicy: Always
         env:
          - name: BROKER_SERVICE
            value: kafka-kafka-0.kafka-svc:9092
EOF
#+end_src

#+RESULTS:

*** Check the logs
#+begin_src sh
  kubectl logs $(kubectl get pods -l app=kudo-kafka-consumer -o jsonpath='{.items[0].metadata.name}') --follow
#+end_src

#+begin_example
  Message: b'2019-07-11T16:28:45Z;0;6;4283'
  Message: b'2019-07-11T16:28:46Z;1;8;4076'
  Message: b'2019-07-11T16:28:47Z;5;2;9140'
  Message: b'2019-07-11T16:28:48Z;5;8;8603'
  Message: b'2019-07-11T16:28:49Z;1;0;5097'
#+end_example

*** Explore KUDO concepts
KUDO is creating new objects in Kubernetes called Custom Resource Definitions
(CRDs).  You can get information about these objects like you can get
information about pods, deployments, services etc.

Run this command to get the list of CRDs created by KUDO:

#+begin_src sh
  kubectl get crds | grep kudo
#+end_src
#+begin_example
  instances.kudo.dev                               2019-08-21T09:30:46Z
  operators.kudo.dev                               2019-08-21T09:30:45Z
  operatorversions.kudo.dev                        2019-08-21T09:30:45Z
#+end_example

Now list the KUDO instances running using the following command:

#+begin_src sh
  kubectl get instances.kudo.dev
#+end_src
#+begin_example
  NAME    AGE
  kafka   18m
  zk      33m
#+end_example

And get information about the KUDO Kafka instance:

#+begin_src sh
  kubectl get instances.kudo.dev kafka -o yaml
#+end_src

#+begin_example
  apiVersion: kudo.dev/v1alpha1
  kind: Instance
  metadata:
    creationTimestamp: "2019-08-21T13:05:09Z"
    generation: 4
    labels:
      controller-tools.k8s.io: "1.0"
      kudo.dev/operator: kafka
    name: kafka
    namespace: default
    resourceVersion: "35698"
    selfLink: /apis/kudo.dev/v1alpha1/namespaces/default/instances/kafka
    uid: 2feaf384-6b4a-4c30-b5ec-4abcb814979b
  spec:
    operatorVersion:
      name: kafka-0.1.2
  status:
    activePlan:
      apiVersion: kudo.dev/v1alpha1
      kind: PlanExecution
      name: kafka-deploy-975266742
      namespace: default
      uid: 33331fe8-e8cc-4eac-b60e-dbfff894ca3d
    status: COMPLETE
#+end_example

This is also the approach you take to delete a running instance (=kubectl delete
instances.kudo.dev kafka=), but you can keep it running.

*** Upgrade Kafka Version and Configuration
Upgrade your Kafka cluster to 2.3.0 (the version of the KUDO Kafka
operator is 0.2.0) using the following command:

#+begin_src sh
  kubectl kudo upgrade kafka --version=0.2.0 --instance kafka
#+end_src
#+begin_example
  operatorversion.kudo.dev/v1alpha1/kafka-0.2.0 successfully created
  instance./kafka successfully updated
#+end_example

Check the status of the upgrade:

#+begin_src sh
  kubectl kudo plan status --instance=kafka
#+end_src
#+begin_example
  Plan(s) for "kafka" in namespace "default":
  .
  └── kafka (Operator-Version: "kafka-0.2.0" Active-Plan: "kafka-deploy-857547438")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase deploy-kafka (serial strategy) [COMPLETE]
      │       └── Step deploy (COMPLETE)
      └── Plan not-allowed (serial strategy) [NOT ACTIVE]
          └── Phase not-allowed (serial strategy) [NOT ACTIVE]
              └── Step not-allowed (serial strategy) [NOT ACTIVE]
                  └── not-allowed [NOT ACTIVE]
#+end_example

And get information about the upgraded KUDO Kafka instance:

#+begin_src sh
  kubectl get instances.kudo.dev kafka -o yaml
#+end_src
#+begin_example
  apiVersion: kudo.dev/v1alpha1
  kind: Instance
  metadata:
    creationTimestamp: "2019-08-21T13:05:09Z"
    generation: 6
    labels:
      controller-tools.k8s.io: "1.0"
      kudo.dev/operator: kafka
    name: kafka
    namespace: default
    resourceVersion: "35828"
    selfLink: /apis/kudo.dev/v1alpha1/namespaces/default/instances/kafka
    uid: 2feaf384-6b4a-4c30-b5ec-4abcb814979b
  spec:
    operatorVersion:
      name: kafka-0.2.0
  status:
    activePlan:
      apiVersion: kudo.dev/v1alpha1
      kind: PlanExecution
      name: kafka-deploy-857547438
      namespace: default
      uid: c2163e4a-b3a0-4889-b8dd-0953c6e4bead
    status: COMPLETE
#+end_example

And check that the corresponding Pods have been replaced:

#+begin_src sh
kubectl get pods | grep kafka
#+end_src
#+begin_example
kafka-kafka-0                          1/1     Running   0          3m33s
kafka-kafka-1                          1/1     Running   0          88s
kafka-kafka-2                          1/1     Running   0          12s
#+end_example



You can also easily update the configuration of your Kafka cluster.

For example, you can add more brokers using the command below.

#+begin_src sh
  kubectl patch instance kafka -p '{"spec":{"parameters":{"BROKER_COUNT":"5"}}}' --type=merge
#+end_src
#+begin_example
  instance.kudo.dev/kafka patched
#+end_example

Check the status of the upgrade:

#+begin_src sh
  kubectl kudo plan status --instance=kafka
#+end_src

#+begin_example
  Plan(s) for "kafka" in namespace "default":
  .
  └── kafka (Operator-Version: "kafka-0.2.0" Active-Plan: "kafka-deploy-294386986")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase deploy-kafka (serial strategy) [COMPLETE]
      │       └── Step deploy (COMPLETE)
      └── Plan not-allowed (serial strategy) [NOT ACTIVE]
          └── Phase not-allowed (serial strategy) [NOT ACTIVE]
              └── Step not-allowed (serial strategy) [NOT ACTIVE]
                  └── not-allowed [NOT ACTIVE]
#+end_example

And check that the corresponding pods are running:

#+begin_src sh
  kubectl get pods | grep kafka
#+end_src

#+begin_example
  kafka-kafka-0                          1/1     Running   0          34s
  kafka-kafka-1                          1/1     Running   0          54s
  kafka-kafka-2                          1/1     Running   0          104s
  kafka-kafka-3                          1/1     Running   0          2m50s
  kafka-kafka-4                          1/1     Running   0          2m27s
  kudo-kafka-consumer-6b4dd5cd59-xs6hn   1/1     Running   0          3h34m
  kudo-kafka-generator-d655d6dff-mx9fz   1/1     Running   0          3h34m
#+end_example

* 8. Scale a Konvoy cluster
  :PROPERTIES:
  :CUSTOM_ID: scale-a-konvoy-cluster
  :END:

Scale the Konvoy cluster to 6 nodes using CD/CD

** Lab Steps

Edit the =cluster.yaml= file to change the worker count from 5 to 6:

  #+begin_src yaml
  nodePools:
  - name: worker
    count: 6
  #+end_src

And run =./konvoy up --yes= again.

Check that there are now 6 kubelets deployed:

#+begin_src sh
  kubectl get nodes
#+end_src

#+begin_example
  NAME                                         STATUS   ROLES    AGE    VERSION
  ip-10-0-128-127.us-west-2.compute.internal   Ready    <none>   45m    v1.15.1
  ip-10-0-129-21.us-west-2.compute.internal    Ready    <none>   45m    v1.15.1
  ip-10-0-129-33.us-west-2.compute.internal    Ready    <none>   2m2s   v1.15.1
  ip-10-0-130-39.us-west-2.compute.internal    Ready    <none>   45m    v1.15.1
  ip-10-0-131-155.us-west-2.compute.internal   Ready    <none>   45m    v1.15.1
  ip-10-0-131-252.us-west-2.compute.internal   Ready    <none>   45m    v1.15.1
  ip-10-0-194-48.us-west-2.compute.internal    Ready    master   48m    v1.15.1
  ip-10-0-194-91.us-west-2.compute.internal    Ready    master   46m    v1.15.1
  ip-10-0-195-21.us-west-2.compute.internal    Ready    master   47m    v1.15.1
#+end_example

* 9. Konvoy monitoring
  :PROPERTIES:
  :CUSTOM_ID: konvoy-monitoring
  :END:

In Konvoy, all the metrics are stored in a Prometheus cluster and
exposed through Grafana.

** Lab Steps
To access the Grafana UI, click on the =Grafana Metrics= icon on the
Konvoy UI.

Take a look at the different Dashboards available.

#+CAPTION: Grafana UI
[[file:images/grafana.png]]

You can also access the Prometheus UI to see all the metrics available
by clicking on the =Prometheus= icon on the Konvoy UI.

#+CAPTION: Prometheus UI
[[file:images/prometheus.png]]


*** Jenkins
	To see a dashboard of our Jenkins instance installed earlier, go to the
	Grafana UI and import [[https://grafana.com/grafana/dashboards/6479][Jenkins Dashboard #6479]] or [[https://grafana.com/grafana/dashboards/9964][Jenkins: Performance and Health Overview #9964]]

*** Kafka
The KUDO Kafka operator comes by default with the JMX Exporter agent enabled.

When the Kafka operator is deployed with parameter =METRICS_ENABLED=true=
(which defaults to =true=) then each broker:

- bootstraps with the [[https://github.com/prometheus/jmx_exporter][JMX Exporter]] java agent exposing the metrics at =9094/metrics=
- adds a port named =metrics= to the Kafka service
- adds a label =kubeaddons.mesosphere.io/servicemonitor: "true"= for the service
  monitor discovery.

Run the following command to enable Kafka metrics export:

#+begin_src sh
  kubectl create -f https://raw.githubusercontent.com/kudobuilder/operators/master/repository/kafka/docs/v0.1/resources/service-monitor.yaml
#+end_src

#+RESULTS:

In the Grafana UI, click on the + sign on the left and select =Import=.

Copy the content of this
[[https://raw.githubusercontent.com/kudobuilder/operators/master/repository/kafka/docs/v0.2/resources/grafana-dashboard.json][file]]
as shown in the picture below.

#+CAPTION: Grafana import
[[file:images/grafana-import.png]]

Click on =Load=.

#+CAPTION: Grafana import data source
[[file:images/grafana-import-data-source.png]]

Select =Prometheus= in the =Prometheus= field and click on =Import=.

#+CAPTION: Grafana Kafka
[[file:images/grafana-kafka.png]]

* 10. Konvoy logging/debugging
  :PROPERTIES:
  :CUSTOM_ID: konvoy-loggingdebugging
  :END:

In Konvoy, all the logs are stored in an Elasticsearch cluster and
exposed through Kibana.

** Lab Steps

In Konvoy, all the logs are stored in Elasticsearch and exposed through Kibana.

To access the Kibana UI, click on the =Kibana Logs= icon on the Konvoy
UI.

#+CAPTION: Kibana UI
[[file:images/kibana.png]]

By default, it only shows the logs for the latest 15 minutes.

Click on the top right corner and select =Last 24 hours=.

Then, search for =redis=:

#+CAPTION: Kibana Redis
[[file:images/kibana-Redis.png]]

You'll see all the logs related to the Redis pod and Service you
deployed previously.

*** 10.1. Ingress troubleshooting using logging

In this section, we will leverage Konvoy logging to troubleshoot Ingress failure issue.

We will deploy an NGINX application and expose it via L7 loadbalancer. The application can be accessed with URLs follows below patten.

#+begin_src sh
http[s]://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/nginx/
#+end_src


+ 1st, let's deploy a nginx application and scale it to 3

#+begin_src bash
kubectl run --image=nginx --replicas=3 --port=80 --restart=Always nginx
#+end_src
+ 2nd, expose a in cluster service

#+begin_src bash
kubectl expose deploy nginx --port 8080 --target-port 80 --type NodePort --name "svc-nginx"
#+end_src
+ 3rd, create a ingress to expose service via Layer7 LB

#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-root
  namespace: default
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: svc-nginx
          servicePort: 8080
        path:  /applications/nginx/
EOF
#+end_src
+ 4th, Now check Ingress configure in Traefik

#+caption: Traefik nginx
file:images/trafik_nginx.png

The =Traefik dashboard= indicates the nginx application is ready to receive traffic but if you try access nginx with URL listed below, you will notice =404 Not Found= error like:

#+begin_src bash
curl -k https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/nginx/
#+end_src

Don't forget the trailing slash at the end of the URL. Otherwise, you won't generate a 404 error.

#+caption: Traefix nginx
file:images/trafik_404.png

Let's troubleshoot this failure with Konvoy Kibana.

#+caption: Kibana nginx
file:images/kibana_nginx.png

With Konvoy Kibana's near real time log collection and indexing, we can easily identify the ingress traffic was eventually handled by a pod =kubernetes.pod_name:nginx-755464dd6c-dnvp9= in nginx service. The log also gave us more information on the failure, ="GET /applications/nginx/ HTTP/1.1" 404=, which tell us that nginx can't find resource at path =/applications/nginx/=.

That is neat! Because w/o Kibana, you wouldn't know which Pod in our nginx service handles this request. (Our nginx deployment example launched 3 Pods to serve HTTP request) Not mention if there are multiple nginx service exists in the same K8s cluster but hosted at different namespace.

To fix this failure requires some knownledge on Nginx configuration. In general, when nginx is launched with default configuration, it serves a virtual directory on its =ROOT= path =(/)=. When receives HTTP requests, the nginx walk through its virtual directory to return back resources to the client.

In terms of out example, the =Ingress= configuration we submitted to k8s was configured to a path at =/applications/nginx/=. The =traefik= ingress controller sees this =Ingress configuration= and forwards any resource request at path =/applications/nginx/= to the down stream nginx service at the same path. The pod =kubernetes.pod_name:nginx-755464dd6c-dnvp9= received this request but nginx instance in this pod failed to locate any resource under path =/applications/nginx/=. That is the reason we saw this failure, ="GET /applications/nginx/ HTTP/1.1" 404=.  

You can, of course, configure nginx instance to serve resources at path =/applications/nginx/=. But an alternative solution is leverage =traefik= to strip PATH =/applications/nginx/= to =ROOT (/)= before route requests to nginx.

According to =Traefik= documentation [PathPrefixStrip](https://docs.traefik.io/configuration/backends/kubernetes/), the annotation =(traefik.ingress.kubernetes.io/rule-type)= is exactly what we need to direct traefik to strip ingress HOST PATH to ROOT PATH forementioned.

To update =Ingress=, we can use below command.

#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    traefik.frontend.rule.type: PathPrefixStrip
  name: nginx-root
  namespace: default
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: svc-nginx
          servicePort: 8080
        path:  /applications/nginx/
EOF
#+end_src
#+caption: Dashboard nginx
file:images/trafik_nginx_200.png

* 11. Upgrade a Konvoy cluster
  :PROPERTIES:
  :CUSTOM_ID: upgrade-a-konvoy-cluster
  :END:

Update the =~/.aws/credentials= file with the new information provided
by your instructor.

** Lab Steps
Edit the =cluster.yaml= file to change the Kubernetes version from
=1.15.1= to =1.15.2= in the 2 corresponding fields:

#+begin_src yaml
  ...
  spec:
    kubernetes:
      version: 1.15.2
  ...
    - name: worker
    addons:
      configVersion: stable-1.15.2-0
  ...
#+end_src

#+begin_src sh
  ./konvoy up --yes --upgrade --force-upgrade
#+end_src

#+begin_example

  This process will take about 15 minutes to complete (additional time may be required for larger clusters)

  STAGE [Provisioning Infrastructure]

  Initializing provider plugins...

  Terraform has been successfully initialized!
  Refreshing Terraform state in-memory prior to plan...
  The refreshed state will be used to calculate this plan, but will not be
  persisted to local or remote state storage.

  random_id.id: Refreshing state... (ID: jKY)

  ...

  No changes. Infrastructure is up-to-date.

  This means that Terraform did not detect any differences between your
  configuration and real physical resources that exist. As a result, no
  actions need to be performed.

  Apply complete! Resources: 0 added, 0 changed, 0 destroyed.

  Outputs:

  cluster_name = konvoy_v1.1.1-8ca6
  vpc_id = vpc-0941bb098eb24080d

  STAGE [Running Preflights]

  ...

  STAGE [Determining Upgrade Safety]

  ip-10-0-193-118.us-west-2.compute.internal                             [OK]
  ip-10-0-193-232.us-west-2.compute.internal                             [OK]
  ip-10-0-194-21.us-west-2.compute.internal                              [OK]
  ip-10-0-128-239.us-west-2.compute.internal                             [WARNING]
    - All replicas of the ReplicaSet "default/Nginx-7c45b84548" are running on this node.
  ip-10-0-128-64.us-west-2.compute.internal                              [WARNING]
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "plugins", which is unsafe for upgrades.
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "tmp", which is unsafe for upgrades.
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "plugin-dir", which is unsafe for upgrades.
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "secrets-dir", which is unsafe for upgrades.
    - Pod "default/http-echo-2" is not being managed by a controller. Upgrading this node might result in data or availability loss.
    - Pod managed by ReplicaSet "default/jenkins-c79f457cb" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/jenkins-c79f457cb" are running on this node.
    - Pod managed by ReplicaSet "default/kudo-kafka-generator-d655d6dff" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/kudo-kafka-generator-d655d6dff" are running on this node.
  ip-10-0-129-247.us-west-2.compute.internal                             [WARNING]
    - Pod "default/http-echo-1" is not being managed by a controller. Upgrading this node might result in data or availability loss.
    - Pod managed by StatefulSet "kudo-system/kudo-controller-manager" is running on this node, and the StatefulSet does not have a replica count greater than 1.
  ip-10-0-129-41.us-west-2.compute.internal                              [OK]
  ip-10-0-129-88.us-west-2.compute.internal                              [WARNING]
    - Pod managed by ReplicaSet "default/ebs-dynamic-app-68b598758" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/ebs-dynamic-app-68b598758" are running on this node.
  ip-10-0-130-84.us-west-2.compute.internal                              [WARNING]
    - Pod managed by ReplicaSet "default/kudo-kafka-consumer-6b4dd5cd59" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/kudo-kafka-consumer-6b4dd5cd59" are running on this node.
    - Pod "default/Redis" is not being managed by a controller. Upgrading this node might result in data or availability loss.

  STAGE [Upgrading Kubernetes]

  ...

  PLAY [Upgrade Nodes] ********************************************************************************************************************************************************************

  ...

  TASK [kubeadm-upgrade-nodes : drain node] ***********************************************************************************************************************************************
  changed: [10.0.129.184 -> ec2-54-191-70-155.us-west-2.compute.amazonaws.com]

  ...

  STAGE [Deploying Enabled Addons]
  helm                                                                   [OK]
  dashboard                                                              [OK]
  awsebscsiprovisioner                                                   [OK]
  opsportal                                                              [OK]
  fluentbit                                                              [OK]
  traefik                                                                [OK]
  kommander                                                              [OK]
  elasticsearch                                                          [OK]
  prometheus                                                             [OK]
  traefik-forward-auth                                                   [OK]
  dex                                                                    [OK]
  prometheusadapter                                                      [OK]
  kibana                                                                 [OK]
  elasticsearchexporter                                                  [OK]
  velero                                                                 [OK]
  dex-k8s-authenticator                                                  [OK]

  STAGE [Removing Disabled Addons]

  Kubernetes cluster and addons deployed successfully!

  Run `./konvoy apply kubeconfig` to update kubectl credentials.

  Navigate to the URL below to access various services running in the cluster.
    https://a1efd30f824244733adc1fb95157b9b1-2077667181.us-west-2.elb.amazonaws.com/ops/landing
  And login using the credentials below.
    Username: angry_williams
    Password: TNFGnFrZjhqaF0SNLoCzN3gvqrEsviTYxvMyuPv8KHU13ob6eNa0N7LfSVhd07Xk

  If the cluster was recently created, the dashboard and services may take a few minutes to be accessible.
#+end_example

If there is any error during the upgrade, run the
=./konvoy up --yes --upgrade --force-upgrade= again. It can happen when
the =drain= command times out.

Without the =--force-upgrade= flag, the Kubernetes nodes that have under
replicated pods wouldn't be upgraded.

Check the version of Kubernetes:

#+begin_src sh
  kubectl get nodes
#+end_src
pp#+begin_example
  NAME                                         STATUS   ROLES    AGE   VERSION
  ip-10-0-128-127.us-west-2.compute.internal   Ready    <none>   80m   v1.15.2
  ip-10-0-129-21.us-west-2.compute.internal    Ready    <none>   80m   v1.15.2
  ip-10-0-129-33.us-west-2.compute.internal    Ready    <none>   36m   v1.15.2
  ip-10-0-130-39.us-west-2.compute.internal    Ready    <none>   80m   v1.15.2
  ip-10-0-131-155.us-west-2.compute.internal   Ready    <none>   80m   v1.15.2
  ip-10-0-131-252.us-west-2.compute.internal   Ready    <none>   80m   v1.15.2
  ip-10-0-194-48.us-west-2.compute.internal    Ready    master   82m   v1.15.2
  ip-10-0-194-91.us-west-2.compute.internal    Ready    master   81m   v1.15.2
  ip-10-0-195-21.us-west-2.compute.internal    Ready    master   82m   v1.15.2
#+end_example

Check that the Redis and the http-echo apps are still accessible

#+begin_src sh
  telnet $(kubectl get svc Redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src

#+begin_src sh
  curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
  curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

* 12. Destroy a Konvoy cluster
  :PROPERTIES:
  :CUSTOM_ID: destroy-a-konvoy-cluster
  :END:

When you run konvoy down, the command removes all of the AWS infrastructure
resources create for the cluster, including any volumes that are backing
PersistentVolumesClaims with a Delete ReclaimPolicy.

To completely remove Konvoy cluster resources:

Change to the directory that contains your cluster’s state files, then run the following command:
#+begin_src sh
cd ~/lab
konvoy down --yes
#+end_src

The konvoy down command then begins removing cluster resources by deleting load
balancers, security groups and volumes. It deletes these resources using the AWS
API to ensure they are deleted quickly.

After konvoy down removes these resources, it uses Terraform to delete the
resources created by the =konvoy up= command and Terraform provisioning.

* 13. Bonus Lab: Ingress troubleshooting
   :PROPERTIES:
   :CUSTOM_ID: ingress-troubleshooting
   :END:

In this section, we will leverage Konvoy logging to troubleshoot Ingress
failure issue.

We will deploy an Nginx application and expose it via L7 load balancer.
The application can be accessed with URLs follows below pattern.

=http[s]://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/Nginx/=

** Lab Steps
- first, let's deploy an Nginx application and scale it to 3 instances (replicas)

#+begin_src sh
  kubectl run --image=Nginx --replicas=3 --port=80 --restart=Always Nginx
#+end_src

- 2nd, expose a in cluster service

#+begin_src sh
  kubectl expose deploy Nginx --port 8080 --target-port 80 --type NodePort --name "svc-Nginx"
#+end_src

- 3rd, create ingress to expose service via Layer7 load balancer

#+begin_src sh
  cat << EOF | kubectl apply -f -
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: Nginx-root
    namespace: default
  spec:
    rules:
    - http:
        paths:
        - backend:
            serviceName: svc-Nginx
            servicePort: 8080
          path:  /applications/Nginx/
  EOF
#+end_src

- 4th, Now check Ingress configure in Traefik

#+CAPTION: Traefik Nginx
[[file:images/trafik_Nginx.png]]

The =Traefik dashboard= indicates the Nginx application is ready to
receive traffic but if you try access Nginx with URL listed below, you
will notice =404 Not Found= error like:

#+begin_src sh
  curl -k https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/Nginx/
#+end_src


#+CAPTION: Traefik Nginx
[[file:images/trafik_404.png]]

Let's troubleshoot this failure with Konvoy Kibana.

#+CAPTION: Kibana Nginx
[[file:images/kibana_Nginx.png]]

With Konvoy Kibana's near real time log collection and indexing, we can
easily identify that the ingress traffic was eventually handled by a pod
=kubernetes.pod_name:Nginx-755464dd6c-dnvp9= in Nginx service. The log
also gave us more information on the failure,
="GET /applications/Nginx/ HTTP/1.1" 404=, which tell us that Nginx
can't find resource at path =/applications/Nginx/=.

That is neat! Because w/o Kibana, you wouldn't know which pod in our Nginx
service handled this request. (Our Nginx deployment example launched 3 pods to
serve HTTP requests).  This could be even more complex if there are multiple
Nginx services existing on the same K8s cluster but hosted at different
namespaces.

To fix this failure requires some knowledge of Nginx configuration. In
general, when Nginx is launched with default configuration, it serves a
virtual directory on its =ROOT= path =(/)=. When it receives HTTP requests,
Nginx walks through its virtual directory to return resources to
the client.

In terms of our example, the =Ingress= configuration we submitted to k8s
was configured to a path at =/applications/Nginx/=. The =traefik=
ingress controller sees this =Ingress configuration= and forwards any
resource requests at path =/applications/Nginx/= to the downstream Nginx
service at the same path. The pod
=kubernetes.pod_name:Nginx-755464dd6c-dnvp9= received this request but the
Nginx instance in this pod failed to locate any resource under path
=/applications/Nginx/=. That is the reason we saw this failure,
="GET /applications/Nginx/ HTTP/1.1" 404=.

You can, of course, configure Nginx instance to serve resources at path
=/applications/Nginx/=. But an alternative solution is to leverage
=traefik= to strip PATH =/applications/Nginx/= to =ROOT (/)= before
routing requests to Nginx.

According to the =Traefik= documentation for [[https://docs.traefik.io/configuration/backends/kubernetes/][PathPrefixStrip]],
the annotation =(traefik.ingress.kubernetes.io/rule-type)= is exactly
what we need to direct traefik to strip ingress HOST PATH to ROOT PATH
aforementioned.

To update =Ingress=, we can use the command below.

#+begin_src sh
  cat << EOF | kubectl apply -f -
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      traefik.frontend.rule.type: PathPrefixStrip
    name: Nginx-root
    namespace: default
  spec:
    rules:
    - http:
        paths:
        - backend:
            serviceName: svc-Nginx
            servicePort: 8080
          path:  /applications/Nginx/
  EOF
#+end_src

#+CAPTION: dashboard Nginx
[[file:images/trafik_Nginx_200.png]]

#  LocalWords:  Jone noexport SETUPFILE num pri tex io src ps awscli sudo cp mv
# LocalWords:  aws mkdir sts UserId Arn keygen kubernetes chmod xvf kubeconfig zk
# LocalWords:  zsh addons init yaml availabilityZones YOUREMAIL nodePools svc pvc
# LocalWords:  configVersion apiVersion containerPort targetPort hashicorp http
# LocalWords:  NodePort backend serviceName servicePort kubeaddons NetworkPolicy
# LocalWords:  podSelector policyTypes matchLabels StorageClass creationTimestamp
# LocalWords:  awsebscsiprovisioner resourceVersion selfLink uid reclaimPolicy
# LocalWords:  volumeBindingMode WaitForFirstConsumer accessModes ReadWriteOnce
# LocalWords:  storageClassName Namespace Finalizers VolumeMode Filesystem awk
# LocalWords:  persistentvolume volumeMounts mountPath persistentVolumeClaim exe
# LocalWords:  claimName apache operatorVersion activePlan PlanExecution URIs idp
# LocalWords:  ClusterRoleBinding roleRef ClusterRole apiGroup clientSecret kube
# LocalWords:  redirectURI userIDKey userNameKey config vpc Preflights ReplicaSet
# LocalWords:  StatefulSet PathPrefixStrip namespace

* Appendix 1: Setting up an external identity provider
  :PROPERTIES:
  :CUSTOM_ID: setting-up-an-external-identity-provider
  :END:

Your Konvoy cluster contains a Dex instance which serves as an identity
broker and allows you to integrate with Google's OAuth.

Google's OAuth 2.0 APIs can be used for both authentication and
authorization.

** Lab Steps

Go to [[https://console.developers.google.com/][Google's developer console]] and create a project.

Select that project.

In the Credentials tab of that project start with setting up the OAuth
consent screen.

Indicate an =Application name= and add the DNS name by which your
Konvoy cluster is publicly reachable (=<public-cluster-dns-name>=) into
=Authorized domains=.

Save the OAuth consent screen configuration.

Press Create credentials, select OAuth client ID, and then Web
application.

Under Authorized redirect URIs insert =https://<public-cluster-dns-name>/dex/callback=.

#+CAPTION: google-idp-application
[[file:images/google-idp-application.png]]

Save the configuration and note down the client ID and the client
secret.

#+CAPTION: google-idp-credentials
[[file:images/google-idp-credentials.png]]

Run the following command (after inserting your email address) to
provide admin rights to your Google account:

#+begin_src sh
  cat <<EOF | kubectl create -f -
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: admin-binding
  subjects:
  - kind: User
    name: <your Google email>
  roleRef:
    kind: ClusterRole
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
  EOF
#+end_src

Update the =~/.aws/credentials= file with the new information provided
by your instructor.

Edit the =cluster.yaml= file and update the =dex= section as below:

#+begin_src yaml
      - name: dex
        enabled: true
        values: |
          config:
            connectors:
            - type: oidc
              id: google
              name: Google Accounts
              config:
                issuer: https://accounts.google.com
                clientID: <client ID>
                clientSecret: <client secret>
                redirectURI: https://<public-cluster-dns-name>/dex/callback
                userIDKey: email
                userNameKey: email
#+end_src

And run =./konvoy up --yes= again to apply the change.

When the update is finished, Go to
=https://<public-cluster-dns-name>/token= and login with your Google
Account.

#+CAPTION: google-idp-token
[[file:images/google-idp-token.png]]

Follow the instructions in the page, but use the command below in the
second step to get the right value for the =server= parameter:

#+begin_src sh
  kubectl config set-cluster kubernetes-cluster \
      --certificate-authority=${HOME}/.kube/certs/kubernetes-cluster/k8s-ca.crt \
      --server=$(kubectl config view | grep server | awk '{ print $2 }')
#+end_src

Run the following command to check that you can administer the Kubernetes
cluster with your Google account:

#+begin_src sh
  kubectl get nodes
#+end_src

