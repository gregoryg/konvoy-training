# -*- fill-column: 80; -*-
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:t date:t e:t email:nil f:t inline:t num:nil
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:1
#+options: todo:t |:t
#+title: Konvoy Training Workshop
#+date: <2019-08-22 Thu>
#+author: D2iQ SE Team
#+instructors: Greg Grubbs, Valerian Jone
#+slide_header: github.com/mesosphere/konvoy-training
#+email: ggrubbs@d2iq.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 26.2 (Org mode 9.2.3)
#+SETUPFILE: /Users/gregj/projects/org-html-themes/setup/theme-bigblow.setup
# #+SETUPFILE: https://raw.githubusercontent.com/fniessen/org-html-themes/master/setup/theme-readtheorg.setup
* Introduction
  :PROPERTIES:
  :CUSTOM_ID: introduction
  :UNNUMBERED: t
  :END:

During this training, you'll learn how to deploy Konvoy and to use its
main features.

* Table of Contents                                                   :TOC_1:

- [[#introduction][Introduction]]
- [[#prerequisites][Prerequisites]]
- [[#deploy-a-konvoy-cluster][1. Deploy a Konvoy cluster]]
- [[#expose-a-kubernetes-application-using-a-service-type-load-balancer-l4][2. Expose a Kubernetes Application using a Service Type Load Balancer (L4)]]
- [[#expose-a-kubernetes-application-using-an-ingress-l7][3. Expose a Kubernetes Application using an Ingress (L7)]]
- [[#leverage-network-policies-to-restrict-access][4. Leverage Network Policies to restrict access]]
- [[#leverage-persistent-storage-using-csi][5. Leverage persistent storage using CSI]]
- [[#deploy-jenkins-using-helm][6. Deploy Jenkins using Helm]]
- [[#deploy-apache-kafka-using-kudo][7. Deploy Apache Kafka using KUDO]]
- [[#scale-a-konvoy-cluster][8. Scale a Konvoy cluster]]
- [[#konvoy-monitoring][9. Konvoy monitoring]]
- [[#konvoy-loggingdebugging][10. Konvoy logging/debugging]]
- [[#setting-up-an-external-identity-provider][11. Setting up an external identity provider]]
- [[#upgrade-a-konvoy-cluster][12. Upgrade a Konvoy cluster]]
- [[#ingress-troubleshooting][13. Bonus Lab: Ingress troubleshooting]]


* Prerequisites
  :PROPERTIES:
  :CUSTOM_ID: prerequisites
  :UNNUMBERED: t
  :END:

** Workshop syllabus
   Please open the page at [[https://github.com/mesosphere/konvoy-training]]


** Workstation or Laptop requirements
You need either a Linux, MacOS or a Windows laptop.

  You may choose to use the
  [[https://console.cloud.google.com/cloudshell][Google Cloud Shell]].

** Software on laptop
*** Docker
If you use your laptop, you need to have Docker installed.

  + MacOS
     [[https://download.docker.com/mac/beta/Docker.dmg][Download Docker Desktop for Mac]]

     The whale in your status bar indicates Docker is running and accessible.

     [[file:images/happy-docker-whale-in-status-bar.png]]
  + Windows
       Click on the [[https://download.docker.com/win/stable/Docker%20for%20Windows%20Installer.exe][Docker Desktop installer .exe file]]

       Run the installer

       Run Docker

       Whale in the task bar tells you it's running!

       [[file:images/docker_icon_taskbar.jpg]]
  + Linux
**** Debian/Ubuntu
      Package name is =docker.io=
      #+begin_src sh
      sudo apt install docker.io
      #+end_src
**** RHEL/Centos
**** After installation

Test that the unprivileged user can use =docker= commands
#+begin_src sh
docker ps
#+end_src
*** AWS Command Line Interface Installation and Setup
You will need to install the AWS CLI.
Either use the Python =pip3= command below or refer to [[https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html][Installing the AWS CLI]]

#+begin_src sh
  pip3 install awscli --upgrade --user
  sudo cp ~/.local/bin/aws /usr/bin/
#+end_src

#+begin_example
  [Temp]
  aws_access_key_id     = xxx
  aws_secret_access_key = xxx
  aws_session_token     = xxx
#+end_example

Add the following information provided by the instructor to the
=~/.aws/credentials= file (or create the file if necessary):
#+begin_src 
mkdir -p ~/.aws

#+end_src

This token will be valid for one hour.

Run the following command to use this profile:

#+begin_src sh
  export AWS_PROFILE=Temp
#+end_src

Test your configuration with the following command at the shell:
#+begin_src sh
aws sts get-caller-identity
#+end_src

The output should be something similar to
#+begin_example
{
    "Account": "110465657741", 
    "UserId": "WEOIQEUTRIUERLU4374CM:ggrubbs@d2iq.com", 
    "Arn": "arn:aws:sts::110465657741:assumed-role/Mesosphere-PowerUser/ggrubbs@d2iq.com"
}
#+end_example

*IMPORTANT*: If you don't finish the deployment on time, the instructor will provide
an updated token.
*** SSH Agent must be working                                      :noexport:
    #+begin_src sh
    ssh-add -l
    #+end_src

    Expected output:
    #+begin_example
    The agent has no identities.
    #+end_example

    If instead, you see
    #+begin_example
    Could not open a connection to your authentication agent.
    #+end_example

    You will need to start the agent.  Easiest way in one single shell session
    is:
    #+begin_src sh
    ssh-agent bash
    ssh-add -l
    #+end_src

    *NOTE* Participant may have to manually create their first SSH key
    #+begin_src sh
    ssh-keygen
    #+end_src
*** =kubectl=                                                      :noexport:
    For any OS, visit [[https://kubernetes.io/docs/tasks/tools/install-kubectl/][Install and Set Up kubectl]] on kubernetes.io

**** After installation
     Verify that =kubectl= was successfully installed and in your path
     #+begin_src sh
     kubectl version
     #+end_src

*** Helm                                                           :noexport:
    To install Helm, visit [[https://github.com/helm/helm/releases/tag/v2.14.3][Helm release page]] and download the =helm= executable for your OS
*** wget/curl                                                      :noexport:

** Jump Servers

Jumpservers have been deployed for each lab participant with all prerequisites
installed. First, go to the participant data spreadsheet and select a host by
entering your name.  Then, download the ssh-private-key (id_rsa_student#) and
change the file permissions.  Finally, ssh to the ipaddress of your assigned
jumpserver using the -i option to specify the identity file to be used.  The
username for the Jumpserver is "centos".

Once you download your SSH key, change the permission on the key
#+begin_src bash
chmod 400 id_rsa_student#
#+end_src

Test your SSH key and access to your jump server
#+begin_src bash
ssh -i id_rsa_student# centos@jumpserver-ip-address
#+end_src

For Windows, you can use the [Google Cloud Shell](https://console.cloud.google.com/cloudshell).
Once your Google Cloud Shell has started, you will have to copy the contents of you id_rsa_student#.pem file to a local file in the cloud shell.  Then change the permission on the file and ssh into the jump host.



#+BEGIN_SRC sh

vi id_rsa_student#
#+END_SRC
#+BEGIN_SRC sh
chmod 400 id_rsa_student#
#+END_SRC

#+BEGIN_SRC sh
ssh -i id_rsa_student# centos@jumpserver-ip-address
#+END_SRC

** Install Konvoy                                                  :noexport:
Clone the GitHub repository and run the following commands to uncompress
the Konvoy binaries:

#+begin_src sh
  tar xvf konvoy_*.tar.bz2
#+end_src

Go to the Konvoy directory and invoke the =konvoy= command:

#+begin_src sh
  cd konvoy_*/
  ./konvoy
#+end_src

The first time you run the =konvoy= command, a docker image will be downloaded
to your machine.  That only happens once per version of Konvoy.  

Your expected output will be something like this:
#+begin_example
Status: Downloaded newer image for mesosphere/konvoy:v1.1.2
docker.io/mesosphere/konvoy:v1.1.2
Usage:
  konvoy [command]

Available Commands:
  apply       Updates certain configuration in the existing kubeconfig file
  check       Run checks on the health of the cluster
  completion  Output shell completion code for the specified shell (bash or zsh)
  deploy      Deploy a fully functioning Kubernetes cluster and addons
  diagnose    Creates a diagnostics bundle of the cluster
  down        Destroy the Kubernetes cluster
  get         Get cluster related information
  help        Help about any command
  init        Create the provision and deploy configuration with default values
  provision   Provision the nodes according to the provided Terraform variables file
  reset       Remove any modifications to the nodes made by the installer, and cleanup file artifacts
  up          Run provision, and deploy (kubernetes, container-networking, and addons) to create or update a Kubernetes cluster reflecting the provided configuration and inventory files
  version     Version for konvoy

Flags:
  -h, --help      help for konvoy
      --version   version for konvoy

Use "konvoy [command] --help" for more information about a command.
#+end_example

* 1. Deploy a Konvoy cluster
  :PROPERTIES:
  :CUSTOM_ID: deploy-a-konvoy-cluster
  :END:

** Objectives
   :PROPERTIES:
   :CUSTOM_ID: objectives
   :END:

- Deploy a Kubernetes cluster with all the addons you need to get a production
  ready container orchestration platform
- Configure kubectl to manage your cluster

** Why is this Important?
   :PROPERTIES:
   :CUSTOM_ID: why-is-this-important
   :END:

There are many ways to deploy a kubernetes cluster from a fully manual procedure
to using a fully automated or opinionated SaaS. Cluster sizes can also widely
vary from a single node deployment on your laptop, to thousands of nodes in a
single logical cluster, or even across multiple clusters. Thus, picking a
deployment model that suits the scale that you need as your business grows is
important.

** Lab Steps
Change directories into the lab directory:

#+begin_src sh
cd ~/lab
#+end_src

Deploy your cluster using the command below:

#+begin_src sh
konvoy up --yes
#+end_src

The output should be similar to:

#+begin_example
  konvoy up --yes                                                                  
  This process will take about 15 minutes to complete (additional time may be required for larger clusters), do you want to continue [y/n]: y

  STAGE [Provisioning Infrastructure]

  Initializing provider plugins...

  ...

  Terraform has been successfully initialized!

  ...

  STAGE [Deploying Enabled Addons]
  helm                                                                   [OK]
  dashboard                                                              [OK]
  fluentbit                                                              [OK]
  awsebscsiprovisioner                                                   [OK]
  traefik                                                                [OK]
  opsportal                                                              [OK]
  kommander                                                              [OK]
  prometheus                                                             [OK]
  elasticsearch                                                          [OK]
  dex                                                                    [OK]
  elasticsearchexporter                                                  [OK]
  kibana                                                                 [OK]
  traefik-forward-auth                                                   [OK]
  prometheusadapter                                                      [OK]
  dex-k8s-authenticator                                                  [OK]
  velero                                                                 [OK]

  STAGE [Removing Disabled Addons]

  Kubernetes cluster and addons deployed successfully!

  Run `./konvoy apply kubeconfig` to update kubectl credentials.

  Navigate to the URL below to access various services running in the cluster.
    https://a7e039f1a05a54f45b36e063f5aee077-287582892.us-west-2.elb.amazonaws.com/ops/landing
  And login using the credentials below.
    Username: goofy_einstein
    Password: tUeARRKxM8PfrIy2cjFc1jI0Hr2I0duzlttr1LzRTKoDooQJ0d1yyutjNv4NLHvy

  If the cluster was recently created, the dashboard and services may take a few minutes to be accessible.
#+end_example

If you get any error during the deployment of the addons (it can happen
with network connectivity issues), then, you can run the following
command to redeploy them:

#+begin_src sh
  konvoy deploy addons --yes
#+end_src

As soon as your cluster is successfully deployed, the URL and the credentials to
access your cluster are displayed.  When you lauch your dashboard URL in your
browser the first screen will ask you to select "login or generate token",
select login and use the credentials provided.

If you need to get this information later, you can execute the command
below:

#+begin_src sh
  konvoy get ops-portal
#+end_src

#+CAPTION: Konvoy UI
[[file:images/konvoy-ui.png]]

Click on the =Kubernetes Dashboard= icon to open it.

#+CAPTION: Kubernetes Dashboard
[[file:images/kubernetes-dashboard.png]]

To configure kubectl to manage your cluster, you simply need to run the
following command:

#+begin_example
  mv ~/.kube/config ~/.kube/config.old
  konvoy apply kubeconfig -force-overwrite
#+end_example

You can check that the Kubernetes cluster has been deployed with 3 control nodes
and 5 worker nodes

#+begin_src sh
  kubectl get nodes
#+end_src
#+begin_example
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-128-64.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-129-247.us-west-2.compute.internal   Ready    <none>   10m   v1.15.2
ip-10-0-129-41.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-129-88.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-130-84.us-west-2.compute.internal    Ready    <none>   10m   v1.15.2
ip-10-0-193-118.us-west-2.compute.internal   Ready    master   11m   v1.15.2
ip-10-0-193-232.us-west-2.compute.internal   Ready    master   12m   v1.15.2
ip-10-0-194-21.us-west-2.compute.internal    Ready    master   13m   v1.15.2
#+end_example


* 2. Expose a Kubernetes Application using a Service Type Load Balancer (L4)
   :PROPERTIES:
   :CUSTOM_ID: expose-a-kubernetes-application-using-a-service-type-load-balancer-l4
   :END:

** Objectives
   :PROPERTIES:
   :CUSTOM_ID: objectives-1
   :END:

- Deploy a Redis pod and expose it using a Service Type Load Balancer
  (L4) and validate that the connection is exposed to the outside
- Deploy a couple hello-world applications and expose them using an
  Ingress service (L7) and validate that the connection is exposed to
  the outside
  [[https://www.webopedia.com/quick_ref/OSI_Layers.asp][The 7 Layers of the OSI Model]]
** Why is this Important?
   :PROPERTIES:
   :CUSTOM_ID: why-is-this-important-1
   :END:

Exposing your application on a kubernetes cluster in an Enterprise-grade
environment can be challenging to set up. With Konvoy, the integration
with AWS cloud load balancer is already done by default and Traefik is
deployed to allow you to easily create ingresses.

** Lab Steps
Deploy a Redis pod on your Kubernetes cluster by running the following
command:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: redis
  name: redis
spec:
  containers:
  - name: redis
    image: redis:5.0.3
    ports:
    - name: redis
      containerPort: 6379
      protocol: TCP
EOF
#+end_src

Then, expose the service, you need to run the following command to
create a Service Type Load Balancer:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Service
metadata:
  labels:
    app: redis
  name: redis
spec:
  type: LoadBalancer
  selector:
    app: redis
  ports:
  - protocol: TCP
    port: 6379
    targetPort: 6379
EOF
#+end_src

Finally, run the following command to see the URL of the Load Balancer
created on AWS for this service:

#+begin_src sh
kubectl get svc redis
#+end_src
#+begin_example
  NAME    TYPE           CLUSTER-IP   EXTERNAL-IP                                                               PORT(S)          AGE
  Redis   LoadBalancer   10.0.51.32   a92b6c9216ccc11e982140acb7ee21b7-1453813785.us-west-2.elb.amazonaws.com   6379:31423/TCP   43s
#+end_example

You need to wait for a few minutes while the Load Balancer is created on
AWS and the name resolution in place.

#+begin_src sh
until nslookup $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname})
do
    sleep 1
done
#+end_src

Expected output:
#+begin_example
  ,** server can't find aa4b038c75236642febfeadf2a1e9e304-1736643327.us-west-2.elb.amazonaws.com: NXDOMAIN

  Server:         169.254.169.254
  Address:        169.254.169.254#53
  ### (above lines repeated)
  Server:         169.254.169.254
  Address:        169.254.169.254#53

  Non-authoritative answer:
  Name:   a4b038c75236642febfeadf2a1e9e304-1736643327.us-west-2.elb.amazonaws.com
  Address: 52.34.37.52
  Name:   a4b038c75236642febfeadf2a1e9e304-1736643327.us-west-2.elb.amazonaws.com
  Address: 54.148.3.99
#+end_example

You can validate that you can access the Redis pod from your laptop
using telnet:

#+begin_src sh
telnet $(kubectl get svc Redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src
#+begin_example
  Trying 52.27.218.48...
  Connected to a92b6c9216ccc11e982140acb7ee21b7-1453813785.us-west-2.elb.amazonaws.com.
  Escape character is '^]'.
  quit
  +OK
  Connection closed by foreign host.
#+end_example

NOTE: To exit =telnet=, type =Control-]=, then =quit=

If you don't have =telnet= installed in your machine, you can use =nc=
instead:

#+begin_src sh
sudo apt install netcat
nc -z $(kubectl get svc Redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379 < /dev/null ; echo $?
#+end_src
* 3. Expose a Kubernetes Application using an Ingress (L7)
  :PROPERTIES:
  :CUSTOM_ID: expose-a-kubernetes-application-using-an-ingress-l7
  :END:

Deploy 2 web application pods on your Kubernetes cluster running the
following command:

#+begin_src sh
  kubectl run --restart=Never --image hashicorp/http-echo --labels app=http-echo-1 --port 80 http-echo-1 -- -listen=:80 --text="Hello from http-echo-1"
  kubectl run --restart=Never --image hashicorp/http-echo --labels app=http-echo-2 --port 80 http-echo-2 -- -listen=:80 --text="Hello from http-echo-2"
#+end_src

Then, expose the pods with a Service Type NodePort using the following
commands:

#+begin_src sh
  kubectl expose pod http-echo-1 --port 80 --target-port 80 --type NodePort --name "http-echo-1"
  kubectl expose pod http-echo-2 --port 80 --target-port 80 --type NodePort --name "http-echo-2"
#+end_src

Create the Ingress to expose the application to the outside
world using the following command:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: echo
spec:
  rules:
  - host: "http-echo-1.com"
    http:
      paths:
      - backend:
          serviceName: http-echo-1
          servicePort: 80
  - host: "http-echo-2.com"
    http:
      paths:
      - backend:
          serviceName: http-echo-2
          servicePort: 80
EOF
#+end_src

Go to the Traefik UI to check that new frontends have been created.

#+CAPTION: Traefik front ends
[[file:images/traefik-frontends.png]]

Finally, run the following command to see the URL of the Load Balancer
created on AWS for the Traefik service:

#+begin_src sh
kubectl get svc traefik-kubeaddons -n kubeaddons
#+end_src
#+BEGIN_EXAMPLE
  NAME                 TYPE           CLUSTER-IP    EXTERNAL-IP                                                             PORT(S)                                     AGE
  traefik-kubeaddons   LoadBalancer   10.0.24.215   abf2e5bda6ca811e982140acb7ee21b7-37522315.us-west-2.elb.amazonaws.com   80:31169/TCP,443:32297/TCP,8080:31923/TCP   4h22m
#+END_EXAMPLE


You can validate that you can access the web application pods from your laptop using the following commands:

#+begin_src sh
curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

You can also set some Traefik annotations to use some advanced features
as described in this
[[https://docs.traefik.io/configuration/backends/kubernetes/][document]].

* 4. Leverage Network Policies to restrict access
  :PROPERTIES:
  :CUSTOM_ID: leverage-network-policies-to-restrict-access
  :END:

By default, all the pods can access all the services inside and outside
the Kubernetes clusters and services exposed to the external world can
be accessed by anyone. Kubernetes Network Policies can be used to
restrict access.

When a Kubernetes cluster is deployed by Konvoy, a Calico cluster is
automatically deployed on this cluster. It allows a user to define
network policies without any additional configuration.

** Objectives
   :PROPERTIES:
   :CUSTOM_ID: objectives-2
   :END:

- Create a network policy to deny any ingress
- Check that the Redis and the http-echo apps aren't accessible anymore
- Create network policies to allow ingress access to these apps only
- Check that the Redis and the http-echo apps are now accessible

** Why is this Important?
   :PROPERTIES:
   :CUSTOM_ID: why-is-this-important-2
   :END:

In many cases, you want to restrict communications between services. For
example, you often want some micro services to be reachable only other specific
micro services.

In this lab, we restrict access to ingresses, so you may think that it's useless
as we can simply not expose these apps if we want to restrict access. But, in
fact, it makes sense to also create network policies to avoid cases where an app
is exposed by mistake.

** Lab Steps
Create a network policy to deny any ingress

#+begin_src sh :session konvoy-sh
cat <<EOF | kubectl create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF
#+end_src

#+RESULTS:

Wait for a minute to allow the network policy to be activated and check
that the Redis and the http-echo apps aren't accessible anymore

#+begin_src sh
telnet $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src

#+begin_src sh
curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

Create network policies to allow ingress access to these apps only

#+begin_src sh
cat <<EOF | kubectl create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-redis
spec:
  podSelector:
    matchLabels:
      app: redis
  ingress:
  - from: []
EOF

cat <<EOF | kubectl create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-http-echo-1
spec:
  podSelector:
    matchLabels:
      app: http-echo-1
  ingress:
  - from: []
EOF

cat <<EOF | kubectl create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-http-echo-2
spec:
  podSelector:
    matchLabels:
      app: http-echo-2
  ingress:
  - from: []
EOF
#+end_src

Wait for a minute and check that the Redis and the http-echo apps are
now accessible

#+begin_src sh
telnet $(kubectl get svc redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src

#+RESULTS:

#+begin_src sh
  curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
  curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

Delete the network policy that denies any ingress

#+begin_src sh
cat <<EOF | kubectl delete -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF
#+end_src

* 5. Leverage persistent storage using CSI
  :PROPERTIES:
  :CUSTOM_ID: leverage-persistent-storage-using-csi
  :END:

** Objectives
   :PROPERTIES:
   :CUSTOM_ID: objectives-3
   :END:

- Create a PersistentVolumeClaim (pvc) to use the AWS EBS CSI driver
- Create a service that will use this PVC and dynamically provision an
  EBS volume
- Validate persistence

** Why is this Important?
   :PROPERTIES:
   :CUSTOM_ID: why-is-this-important-3
   :END:

The goal of CSI is to establish a standardized mechanism for Container
Orchestration Systems to expose arbitrary storage systems to their
containerized workloads. The CSI specification emerged from cooperation
between community members from various Container Orchestration Systems
including Kubernetes, Mesos, Docker, and Cloud Foundry.

By creating an industry standard interface, the CSI initiative sets
ground rules in order to minimize user confusion. By providing a
pluggable standardized interface, the community will be able to adopt
and maintain new CSI-enabled storage drivers to their kubernetes
clusters as they mature. Choosing a solution that supports CSI
integration will allow your business to adopt the latest and greatest
storage solutions with ease.

** Lab Steps 

When Konvoy is deployed on AWS, a =StorageClass= is created
automatically as you can see below.  Note the =WaitForFirstConsumer= value for
the volume binding mode parameter

#+begin_src sh
kubectl get sc awsebscsiprovisioner -o yaml
#+end_src

#+begin_example
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    annotations:
      kubernetes.io/description: AWS EBS CSI provisioner StorageClass
      storageclass.kubernetes.io/is-default-class: "true"
    creationTimestamp: "2019-08-12T10:43:23Z"
    name: awsebscsiprovisioner
    resourceVersion: "1573"
    selfLink: /apis/storage.k8s.io/v1/storageclasses/awsebscsiprovisioner
    uid: 413745a0-ec52-4917-afb5-70bdf8f2a606
  parameters:
    type: gp2
  provisioner: ebs.csi.aws.com
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
#+end_example

Create the Kubernetes PersistentVolumeClaim using the following command:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: awsebscsiprovisioner
  resources:
    requests:
      storage: 1Gi
EOF
#+end_src

Run the following command to check the status of the
=PersistentVolumeClaim=:

#+begin_src sh
kubectl describe pvc dynamic
#+end_src

#+begin_example

  Name:          dynamic
  Namespace:     default
  StorageClass:  awsebscsiprovisioner
  Status:        Pending
  Volume:        
  Labels:        <none>
  Annotations:   <none>
  Finalizers:    [kubernetes.io/pvc-protection]
  Capacity:      
  Access Modes:  
  VolumeMode:    Filesystem
  Events:
    Type       Reason                Age               From                         Message
    ----       ------                ----              ----                         -------
    Normal     WaitForFirstConsumer  3s (x3 over 21s)  persistentvolume-controller  waiting for first consumer to be created before binding
  Mounted By:  <none>
#+end_example

As you can see, it is waiting for a =Pod= to use it to provision the AWS
EBS volume.

Create a Kubernetes Deployment that will use this
=PersistentVolumeClaim= using the following command:

#+begin_src sh
cat <<EOF | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ebs-dynamic-app
  labels:
    app: ebs-dynamic-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ebs-dynamic-app
  template:
    metadata:
      labels:
        app: ebs-dynamic-app
    spec:
      containers:
      - name: ebs-dynamic-app
        image: centos:7
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo \$(date -u) >> /data/out.txt; sleep 5; done"]
        volumeMounts:
        - name: persistent-storage
          mountPath: /data
      volumes:
      - name: persistent-storage
        persistentVolumeClaim:
          claimName: dynamic
EOF
#+end_src

Run the following command until the pod is running:

#+begin_src sh
kubectl get pods
#+end_src

Check the content of the file =/data/out.txt= and note the first
timestamp:

#+begin_src sh
pod=$(kubectl get pods | grep ebs-dynamic-app | awk '{ print $1 }')
kubectl exec -i $pod cat /data/out.txt
#+end_src

Delete the pod using the following command (it will take some time to
complete):

#+begin_src sh
kubectl delete pod $pod
#+end_src

The Deployment will recreate the pod automatically.

Run the following command until the pod is running:

#+begin_src sh
kubectl get pods
#+end_src

Check the content of the file =/data/out.txt= and verify that the first
timestamp is the same as the one noted previously:

#+begin_src sh
pod=$(kubectl get pods | grep ebs-dynamic-app | awk '{ print $1 }')
kubectl exec -i $pod cat /data/out.txt
#+end_src

* 6. Deploy Jenkins using Helm
  :PROPERTIES:
  :CUSTOM_ID: deploy-jenkins-using-helm
  :END:

Helm is a tool for managing Kubernetes charts. Charts are packages of
pre-configured Kubernetes resources.

You can find many charts on the [[https://hub.helm.sh/][Helm Hub]].

** Lab Steps
In this lab, we'll deploy the
[[https://hub.helm.sh/charts/stable/jenkins][Jenkins Helm chart]].

To deploy the chart, you need to run the following command:

#+begin_src sh 
  helm install stable/jenkins --name jenkins --version 1.5.0 --set master.adminPassword=password
#+end_src

#+RESULTS:
#+begin_example
  NAME:   jenkins
  LAST DEPLOYED: Wed Aug  7 17:21:32 2019
  NAMESPACE: default
  STATUS: DEPLOYED

  RESOURCES:
  ==> v1/ConfigMap
  NAME           DATA  AGE
  jenkins        5     1s
  jenkins-tests  1     1s

  ==> v1/Deployment
  NAME     READY  UP-TO-DATE  AVAILABLE  AGE
  jenkins  0/1    1           0          1s

  ==> v1/PersistentVolumeClaim
  NAME     STATUS   VOLUME                CAPACITY  ACCESS MODES  STORAGECLASS  AGE
  jenkins  Pending  awsebscsiprovisioner  1s

  ==> v1/Pod(related)
  NAME                     READY  STATUS   RESTARTS  AGE
  jenkins-c79f457cb-ccttb  0/1    Pending  0         1s

  ==> v1/Role
  NAME                     AGE
  jenkins-schedule-agents  1s

  ==> v1/RoleBinding
  NAME                     AGE
  jenkins-schedule-agents  1s

  ==> v1/Secret
  NAME     TYPE    DATA  AGE
  jenkins  Opaque  2     1s

  ==> v1/Service
  NAME           TYPE          CLUSTER-IP  EXTERNAL-IP  PORT(S)         AGE
  jenkins        LoadBalancer  10.0.9.26   <pending>    8080:30323/TCP  1s
  jenkins-agent  ClusterIP     10.0.41.64  <none>       50000/TCP       1s

  ==> v1/ServiceAccount
  NAME     SECRETS  AGE
  jenkins  1        1s
#+end_example


Then, run the following command to get the URL of the Load Balancer
created on AWS for this service:

#+begin_src sh
  kubectl get svc jenkins
#+end_src

#+begin_example
NAME      TYPE           CLUSTER-IP   EXTERNAL-IP                                                              PORT(S)          AGE
  jenkins   LoadBalancer   10.0.9.26    a71b8025991124a90b2babf7ba2a75da-492974167.us-west-2.elb.amazonaws.com   8080:30323/TCP   16m
#+end_example

You need to wait for a few minutes while the Load Balancer is created on
AWS and the name resolution in place.

#+begin_src sh :results replace verbatim :exports code
until nslookup $(kubectl get svc jenkins --output jsonpath={.status.loadBalancer.ingress[*].hostname})
  do
    sleep 1
  done
  echo "Open http://$(kubectl get svc jenkins --output jsonpath={.status.loadBalancer.ingress[*].hostname}):8080 to access the Jenkins UI"
#+end_src

#+begin_example
Server:		100.115.92.193
Address:	100.115.92.193#53

Non-authoritative answer:
Name:	a3b2b652ab4354e7f8eded17d4323afb-1148896961.us-west-2.elb.amazonaws.com
Address: 52.37.201.113
Name:	a3b2b652ab4354e7f8eded17d4323afb-1148896961.us-west-2.elb.amazonaws.com
Address: 52.34.15.160

Open http://a3b2b652ab4354e7f8eded17d4323afb-1148896961.us-west-2.elb.amazonaws.com:8080 to access the Jenkins UI
#+end_example


Go to the corresponding URL to access Jenkins

Login with the user =admin= and the password =password=.

* 7. Deploy Apache Kafka using KUDO
  :PROPERTIES:
  :CUSTOM_ID: deploy-apache-kafka-using-kudo
  :END:

** Objectives

- Deploy a stateful application that requires orchestration to deploy
  (ZooKeeper, then Kafka)
- Show the usefulness of a purely declarative means of controlling software
  lifecycle using specification, not Go code


** Why is this Important?

The Kubernetes Universal Declarative Operator (KUDO) is a highly productive
toolkit for writing operators for Kubernetes. Using KUDO, you can deploy your
applications, give your users the tools they need to operate it and manage its
lifecyle, and understand how it's behaving in their environments.  

Orchestration is handled purely by declaration in YAML without requiring code
for conditional logic and other orchestration requirements.


** Resources
   Kudo community web site at kudo.dev
   
   Public code hosted at https://github.com/kudobuilder

   Community Slack channel at kubernetes.slack.com #kudo


** Lab Steps
*** Download or clone the KUDO Operators we will use               :noexport:
We will need to have the KUDO operators in order to install ZooKeeper and
Kafka.  

If you use =git=, simply clone the KUDO operators repository
#+begin_src 
git clone https://github.com/kudobuilder/operators.git
#+end_src

Otherwise, download a ZIP file of the directory by visiting
https://github.com/kudobuilder/operators, then clicking on the "Clone or
download" button and selecting "Download ZIP"

*** Install KUDO on the Konvoy cluster

Run the following command to deploy KUDO on your Kubernetes cluster:

#+begin_src sh
kubectl kudo init
#+end_src

Check the status of the KUDO controller:

#+begin_src sh
  kubectl get pods -n kudo-system
#+end_src

#+begin_example
  NAME                        READY   STATUS    RESTARTS   AGE
  kudo-controller-manager-0   1/1     Running   0          84s
#+end_example

*** Install the KUDO CLI                                           :noexport:
    KUDO requires downloading a =kubectl-kudo= command for the command line interface.  
**** MacOS

#+begin_src sh
  brew tap kudobuilder/tap
  brew install kudo-cli
#+end_src
**** Linux
#+begin_src sh
  wget https://github.com/kudobuilder/kudo/releases/download/v0.7.2/kubectl-kudo_0.7.2_linux_x86_64
  sudo mv kubectl-kudo_0.7.2_linux_x86_64 /usr/bin/kubectl-kudo
  chmod +x /usr/bin/kubectl-kudo
#+end_src

**** Verify installation
     #+begin_src sh
     kubectl-kudo version
     kubectl kudo version
     #+end_src

     Expected output from both commands
     #+begin_example
     KUDO Version: version.Info{GitVersion:"0.7.2", GitCommit:"f9f6aa85", BuildDate:"2019-08-02T15:05:56Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"}
     #+end_example

*** Deploy ZooKeeper using KUDO

TODO: grab current output of install for example block

We will need the =operators= directory that you downloaded at the beginning of this lab.

#+begin_src sh
kubectl kudo install zookeeper --instance=zk
#+end_src
#+begin_example
  operator.kudo.k8s.io/v1alpha1/zookeeper created
  operatorversion.kudo.k8s.io/v1alpha1/zookeeper-0.1.0 created
  No instance named 'zk' tied to this 'zookeeper' version has been found. Do you want to create one? (Yes/no) yes
  instance.kudo.k8s.io/v1alpha1/zk created
#+end_example

Check the status of the deployment:

#+begin_src sh
  kubectl kudo plan status --instance=zk
#+end_src
#+begin_example
  Plan(s) for "zk" in namespace "default":
  .
  └── zk (Operator-Version: "zookeeper-0.1.0" Active-Plan: "zk-deploy-694218097")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase zookeeper (parallel strategy) [COMPLETE]
      │       └── Step everything (COMPLETE)
      └── Plan validation (serial strategy) [NOT ACTIVE]
          └── Phase connection (parallel strategy) [NOT ACTIVE]
              └── Step connection (parallel strategy) [NOT ACTIVE]
                  └── connection [NOT ACTIVE]
#+end_example

And check that the corresponding pods are running:

#+begin_src sh
  kubectl get pods | grep zk
#+end_src
#+begin_example
  zk-zookeeper-0                    1/1     Running   0          81s
  zk-zookeeper-1                    1/1     Running   0          81s
  zk-zookeeper-2                    1/1     Running   0          81s
#+end_example

*** Deploy Kafka using KUDO
Deploy Kafka 2.2.1 using KUDO (the version of the KUDO Kafka operator is 0.1.2):

Assuming you cloned or unzipped the directory to =/home/myuser/operators=
#+begin_src sh
  cd /home/myuser/operators/repository/kafka/operator
    kubectl kudo install . --instance=kafka --version=0.1.2
#+end_src

Check the status of the deployment.  This will only show status when deploying, otherwise will report a plan status is not found.

#+begin_src sh
  kubectl kudo plan status --instance=kafka
#+end_src
#+begin_example
  Plan(s) for "kafka" in namespace "default":
  .
  └── kafka (Operator-Version: "kafka-0.1.2" Active-Plan: "kafka-deploy-975266742")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase deploy-kafka (serial strategy) [COMPLETE]
      │       └── Step deploy (COMPLETE)
      └── Plan not-allowed (serial strategy) [NOT ACTIVE]
          └── Phase not-allowed (serial strategy) [NOT ACTIVE]
              └── Step not-allowed (serial strategy) [NOT ACTIVE]
                  └── not-allowed [NOT ACTIVE]
#+end_example

And check that the corresponding pods are running:

#+begin_src sh
  kubectl get pods | grep kafka
#+end_src

#+begin_example
  kafka-kafka-0                          1/1     Running   0          39s
  kafka-kafka-1                          1/1     Running   0          58s
  kafka-kafka-2                          1/1     Running   0          118s
#+end_example

*** Produce messages in Kafka
    This Deployment will continuously place messages on a Kafka topic

#+begin_src sh :session konvoy-sh :results none
cat <<EOF | kubectl create -f -
  apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    name: kudo-kafka-generator
  spec:
    replicas: 1
    template:
      metadata:
        name: kudo-kafka-generator
        labels:
          app: kudo-kafka-generator
      spec:
        containers:
        - name: kudo-kafka-generator
          image: mesosphere/flink-generator:0.1
          command: ["/generator-linux"]
          imagePullPolicy: Always
          args: ["--broker", "kafka-kafka-0.kafka-svc:9092"]
EOF
#+end_src


*** Consume messages from Kafka:

#+begin_src sh :session konvoy-sh :results none
cat <<EOF | kubectl create -f -
  apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
   name: kudo-kafka-consumer
  spec:
   replicas: 1
   template:
     metadata:
       name: kudo-kafka-consumer
       labels:
         app: kudo-kafka-consumer
     spec:
       containers:
       - name: kudo-kafka-consumer
         image: tbaums/kudo-kafka-demo
         imagePullPolicy: Always
         env:
          - name: BROKER_SERVICE
            value: kafka-kafka-0.kafka-svc:9092
EOF
#+end_src

#+RESULTS:

*** Check the logs
#+begin_src sh
  kubectl logs $(kubectl get pods -l app=kudo-kafka-consumer -o jsonpath='{.items[0].metadata.name}') --follow
#+end_src

#+begin_example
  Message: b'2019-07-11T16:28:45Z;0;6;4283'
  Message: b'2019-07-11T16:28:46Z;1;8;4076'
  Message: b'2019-07-11T16:28:47Z;5;2;9140'
  Message: b'2019-07-11T16:28:48Z;5;8;8603'
  Message: b'2019-07-11T16:28:49Z;1;0;5097'
#+end_example

*** Explore KUDO concepts
KUDO is creating new objects in Kubernetes called Custom Resource Definitions
(CRDs).  You can get information about these objects like you can get
information about pods, deployments, services etc.

Run this command to get the list of CRDs created by KUDO:

#+begin_src sh
  kubectl get crds | grep kudo
#+end_src
#+begin_example
  instances.kudo.dev                               2019-08-21T09:30:46Z
  operators.kudo.dev                               2019-08-21T09:30:45Z
  operatorversions.kudo.dev                        2019-08-21T09:30:45Z
#+end_example

Now list the KUDO instances running using the following command:

#+begin_src sh
  kubectl get instances.kudo.dev
#+end_src
#+begin_example
  NAME    AGE
  kafka   18m
  zk      33m
#+end_example

And get information about the KUDO Kafka instance:

#+begin_src sh
  kubectl get instances.kudo.dev kafka -o yaml
#+end_src

#+begin_example
  apiVersion: kudo.dev/v1alpha1
  kind: Instance
  metadata:
    creationTimestamp: "2019-08-21T13:05:09Z"
    generation: 4
    labels:
      controller-tools.k8s.io: "1.0"
      kudo.dev/operator: kafka
    name: kafka
    namespace: default
    resourceVersion: "35698"
    selfLink: /apis/kudo.dev/v1alpha1/namespaces/default/instances/kafka
    uid: 2feaf384-6b4a-4c30-b5ec-4abcb814979b
  spec:
    operatorVersion:
      name: kafka-0.1.2
  status:
    activePlan:
      apiVersion: kudo.dev/v1alpha1
      kind: PlanExecution
      name: kafka-deploy-975266742
      namespace: default
      uid: 33331fe8-e8cc-4eac-b60e-dbfff894ca3d
    status: COMPLETE
#+end_example

This is also the approach you take to delete a running instance (=kubectl delete
instances.kudo.dev kafka=), but you can keep it running.

*** Upgrade Kafka Version and Configuration
Upgrade your Kafka cluster to 2.3.0 (the version of the KUDO Kafka
operator is 0.2.0) using the following command:

#+begin_src sh
  kubectl kudo upgrade kafka --version=0.2.0 --instance kafka
#+end_src
#+begin_example
  operatorversion.kudo.dev/v1alpha1/kafka-0.2.0 successfully created
  instance./kafka successfully updated
#+end_example

Check the status of the upgrade:

#+begin_src sh
  kubectl kudo plan status --instance=kafka
#+end_src
#+begin_example
  Plan(s) for "kafka" in namespace "default":
  .
  └── kafka (Operator-Version: "kafka-0.2.0" Active-Plan: "kafka-deploy-857547438")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase deploy-kafka (serial strategy) [COMPLETE]
      │       └── Step deploy (COMPLETE)
      └── Plan not-allowed (serial strategy) [NOT ACTIVE]
          └── Phase not-allowed (serial strategy) [NOT ACTIVE]
              └── Step not-allowed (serial strategy) [NOT ACTIVE]
                  └── not-allowed [NOT ACTIVE]
#+end_example

And get information about the upgraded KUDO Kafka instance:

#+begin_src sh
  kubectl get instances.kudo.dev kafka -o yaml
#+end_src
#+begin_example
  apiVersion: kudo.dev/v1alpha1
  kind: Instance
  metadata:
    creationTimestamp: "2019-08-21T13:05:09Z"
    generation: 6
    labels:
      controller-tools.k8s.io: "1.0"
      kudo.dev/operator: kafka
    name: kafka
    namespace: default
    resourceVersion: "35828"
    selfLink: /apis/kudo.dev/v1alpha1/namespaces/default/instances/kafka
    uid: 2feaf384-6b4a-4c30-b5ec-4abcb814979b
  spec:
    operatorVersion:
      name: kafka-0.2.0
  status:
    activePlan:
      apiVersion: kudo.dev/v1alpha1
      kind: PlanExecution
      name: kafka-deploy-857547438
      namespace: default
      uid: c2163e4a-b3a0-4889-b8dd-0953c6e4bead
    status: COMPLETE
#+end_example

And check that the corresponding Pods have been replaced:

#+begin_src sh
kubectl get pods | grep kafka
#+end_src
#+begin_example
kafka-kafka-0                          1/1     Running   0          3m33s
kafka-kafka-1                          1/1     Running   0          88s
kafka-kafka-2                          1/1     Running   0          12s
#+end_example



You can also easily update the configuration of your Kafka cluster.

For example, you can add more brokers using the command below.

#+begin_src sh
  kubectl patch instance kafka -p '{"spec":{"parameters":{"BROKER_COUNT":"5"}}}' --type=merge
#+end_src
#+begin_example
  instance.kudo.dev/kafka patched
#+end_example

Check the status of the upgrade:

#+begin_src sh
  kubectl kudo plan status --instance=kafka
#+end_src

#+begin_example
  Plan(s) for "kafka" in namespace "default":
  .
  └── kafka (Operator-Version: "kafka-0.2.0" Active-Plan: "kafka-deploy-294386986")
      ├── Plan deploy (serial strategy) [COMPLETE]
      │   └── Phase deploy-kafka (serial strategy) [COMPLETE]
      │       └── Step deploy (COMPLETE)
      └── Plan not-allowed (serial strategy) [NOT ACTIVE]
          └── Phase not-allowed (serial strategy) [NOT ACTIVE]
              └── Step not-allowed (serial strategy) [NOT ACTIVE]
                  └── not-allowed [NOT ACTIVE]
#+end_example

And check that the corresponding pods are running:

#+begin_src sh
  kubectl get pods | grep kafka
#+end_src

#+begin_example
  kafka-kafka-0                          1/1     Running   0          34s
  kafka-kafka-1                          1/1     Running   0          54s
  kafka-kafka-2                          1/1     Running   0          104s
  kafka-kafka-3                          1/1     Running   0          2m50s
  kafka-kafka-4                          1/1     Running   0          2m27s
  kudo-kafka-consumer-6b4dd5cd59-xs6hn   1/1     Running   0          3h34m
  kudo-kafka-generator-d655d6dff-mx9fz   1/1     Running   0          3h34m
#+end_example

* 8. Scale a Konvoy cluster
  :PROPERTIES:
  :CUSTOM_ID: scale-a-konvoy-cluster
  :END:

Scale the Konvoy cluster to 6 nodes using CD/CD

** Lab Steps

Edit the =cluster.yaml= file to change the worker count from 5 to 6:

  #+begin_src yaml
  nodePools:
  - name: worker
    count: 6
  #+end_src

And run =./konvoy up --yes= again.

Check that there are now 6 kubelets deployed:

#+begin_src sh
  kubectl get nodes
#+end_src

#+begin_example
  NAME                                         STATUS   ROLES    AGE    VERSION
  ip-10-0-128-127.us-west-2.compute.internal   Ready    <none>   45m    v1.15.1
  ip-10-0-129-21.us-west-2.compute.internal    Ready    <none>   45m    v1.15.1
  ip-10-0-129-33.us-west-2.compute.internal    Ready    <none>   2m2s   v1.15.1
  ip-10-0-130-39.us-west-2.compute.internal    Ready    <none>   45m    v1.15.1
  ip-10-0-131-155.us-west-2.compute.internal   Ready    <none>   45m    v1.15.1
  ip-10-0-131-252.us-west-2.compute.internal   Ready    <none>   45m    v1.15.1
  ip-10-0-194-48.us-west-2.compute.internal    Ready    master   48m    v1.15.1
  ip-10-0-194-91.us-west-2.compute.internal    Ready    master   46m    v1.15.1
  ip-10-0-195-21.us-west-2.compute.internal    Ready    master   47m    v1.15.1
#+end_example

* 9. Konvoy monitoring
  :PROPERTIES:
  :CUSTOM_ID: konvoy-monitoring
  :END:

In Konvoy, all the metrics are stored in a Prometheus cluster and
exposed through Grafana.

** Lab Steps
To access the Grafana UI, click on the =Grafana Metrics= icon on the
Konvoy UI.

Take a look at the different Dashboards available.

#+CAPTION: Grafana UI
[[file:images/grafana.png]]

You can also access the Prometheus UI to see all the metrics available
by clicking on the =Prometheus= icon on the Konvoy UI.

#+CAPTION: Prometheus UI
[[file:images/prometheus.png]]


*** Jenkins
	To see a dashboard of our Jenkins instance installed earlier, go to the
	Grafana UI and import [[https://grafana.com/grafana/dashboards/6479][Jenkins Dashboard #6479]] or [[https://grafana.com/grafana/dashboards/9964][Jenkins: Performance and Health Overview #9964]]

*** Kafka
The KUDO Kafka operator comes by default with the JMX Exporter agent enabled.

When the Kafka operator is deployed with parameter =METRICS_ENABLED=true=
(which defaults to =true=) then each broker:

- bootstraps with the [[https://github.com/prometheus/jmx_exporter][JMX Exporter]] java agent exposing the metrics at =9094/metrics=
- adds a port named =metrics= to the Kafka service
- adds a label =kubeaddons.mesosphere.io/servicemonitor: "true"= for the service
  monitor discovery.

Run the following command to enable Kafka metrics export:

#+begin_src sh
  kubectl create -f https://raw.githubusercontent.com/kudobuilder/operators/master/repository/kafka/docs/v0.1/resources/service-monitor.yaml
#+end_src

#+RESULTS:

In the Grafana UI, click on the + sign on the left and select =Import=.

Copy the content of this
[[https://raw.githubusercontent.com/kudobuilder/operators/master/repository/kafka/docs/v0.2/resources/grafana-dashboard.json][file]]
as shown in the picture below.

#+CAPTION: Grafana import
[[file:images/grafana-import.png]]

Click on =Load=.

#+CAPTION: Grafana import data source
[[file:images/grafana-import-data-source.png]]

Select =Prometheus= in the =Prometheus= field and click on =Import=.

#+CAPTION: Grafana Kafka
[[file:images/grafana-kafka.png]]

* 10. Konvoy logging/debugging
  :PROPERTIES:
  :CUSTOM_ID: konvoy-loggingdebugging
  :END:

In Konvoy, all the logs are stored in an Elasticsearch cluster and
exposed through Kibana.

** Lab Steps

In Konvoy, all the logs are stored in Elasticsearch and exposed through Kibana.

To access the Kibana UI, click on the =Kibana Logs= icon on the Konvoy
UI.

#+CAPTION: Kibana UI
[[file:images/kibana.png]]

By default, it only shows the logs for the latest 15 minutes.

Click on the top right corner and select =Last 24 hours=.

Then, search for =redis=:

#+CAPTION: Kibana Redis
[[file:images/kibana-Redis.png]]

You'll see all the logs related to the Redis pod and Service you
deployed previously.

*** 10.1. Ingress troubleshooting using logging

In this section, we will leverage Konvoy logging to troubleshoot Ingress failure issue.

We will deploy an NGINX application and expose it via L7 loadbalancer. The application can be accessed with URLs follows below patten.

#+begin_src sh
http[s]://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/nginx/
#+end_src


+ 1st, let's deploy a nginx application and scale it to 3

#+begin_src bash
kubectl run --image=nginx --replicas=3 --port=80 --restart=Always nginx
#+end_src
+ 2nd, expose a in cluster service

#+begin_src bash
kubectl expose deploy nginx --port 8080 --target-port 80 --type NodePort --name "svc-nginx"
#+end_src
+ 3rd, create a ingress to expose service via Layer7 LB

#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-root
  namespace: default
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: svc-nginx
          servicePort: 8080
        path:  /applications/nginx/
EOF
#+end_src
+ 4th, Now check Ingress configure in Traefik

#+caption: Traefik nginx
file:images/trafik_nginx.png

The =Traefik dashboard= indicates the nginx application is ready to receive traffic but if you try access nginx with URL listed below, you will notice =404 Not Found= error like:

#+begin_src bash
curl -k https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/nginx/
#+end_src

Don't forget the trailing slash at the end of the URL. Otherwise, you won't generate a 404 error.

#+caption: Traefix nginx
file:images/trafik_404.png

Let's troubleshoot this failure with Konvoy Kibana.

#+caption: Kibana nginx
file:images/kibana_nginx.png

With Konvoy Kibana's near real time log collection and indexing, we can easily identify the ingress traffic was eventually handled by a pod =kubernetes.pod_name:nginx-755464dd6c-dnvp9= in nginx service. The log also gave us more information on the failure, ="GET /applications/nginx/ HTTP/1.1" 404=, which tell us that nginx can't find resource at path =/applications/nginx/=.

That is neat! Because w/o Kibana, you wouldn't know which Pod in our nginx service handles this request. (Our nginx deployment example launched 3 Pods to serve HTTP request) Not mention if there are multiple nginx service exists in the same K8s cluster but hosted at different namespace.

To fix this failure requires some knownledge on Nginx configuration. In general, when nginx is launched with default configuration, it serves a virtual directory on its =ROOT= path =(/)=. When receives HTTP requests, the nginx walk through its virtual directory to return back resources to the client.

In terms of out example, the =Ingress= configuration we submitted to k8s was configured to a path at =/applications/nginx/=. The =traefik= ingress controller sees this =Ingress configuration= and forwards any resource request at path =/applications/nginx/= to the down stream nginx service at the same path. The pod =kubernetes.pod_name:nginx-755464dd6c-dnvp9= received this request but nginx instance in this pod failed to locate any resource under path =/applications/nginx/=. That is the reason we saw this failure, ="GET /applications/nginx/ HTTP/1.1" 404=.  

You can, of course, configure nginx instance to serve resources at path =/applications/nginx/=. But an alternative solution is leverage =traefik= to strip PATH =/applications/nginx/= to =ROOT (/)= before route requests to nginx.

According to =Traefik= documentation [PathPrefixStrip](https://docs.traefik.io/configuration/backends/kubernetes/), the annotation =(traefik.ingress.kubernetes.io/rule-type)= is exactly what we need to direct traefik to strip ingress HOST PATH to ROOT PATH forementioned.

To update =Ingress=, we can use below command.

#+begin_src bash
cat << EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    traefik.frontend.rule.type: PathPrefixStrip
  name: nginx-root
  namespace: default
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: svc-nginx
          servicePort: 8080
        path:  /applications/nginx/
EOF
#+end_src
#+caption: Dashboard nginx
file:images/trafik_nginx_200.png

* 11. Setting up an external identity provider
  :PROPERTIES:
  :CUSTOM_ID: setting-up-an-external-identity-provider
  :END:

Your Konvoy cluster contains a Dex instance which serves as an identity
broker and allows you to integrate with Google's OAuth.

Google's OAuth 2.0 APIs can be used for both authentication and
authorization.

** Lab Steps

Go to [[https://console.developers.google.com/][Google's developer console]] and create a project.

Select that project.

In the Credentials tab of that project start with setting up the OAuth
consent screen.

Indicate an =Application name= and add the DNS name by which your
Konvoy cluster is publicly reachable (=<public-cluster-dns-name>=) into
=Authorized domains=.

Save the OAuth consent screen configuration.

Press Create credentials, select OAuth client ID, and then Web
application.

Under Authorized redirect URIs insert =https://<public-cluster-dns-name>/dex/callback=.

#+CAPTION: google-idp-application
[[file:images/google-idp-application.png]]

Save the configuration and note down the client ID and the client
secret.

#+CAPTION: google-idp-credentials
[[file:images/google-idp-credentials.png]]

Run the following command (after inserting your email address) to
provide admin rights to your Google account:

#+begin_src sh
  cat <<EOF | kubectl create -f -
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: admin-binding
  subjects:
  - kind: User
    name: <your Google email>
  roleRef:
    kind: ClusterRole
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
  EOF
#+end_src

Update the =~/.aws/credentials= file with the new information provided
by your instructor.

Edit the =cluster.yaml= file and update the =dex= section as below:

#+begin_src yaml
      - name: dex
        enabled: true
        values: |
          config:
            connectors:
            - type: oidc
              id: google
              name: Google Accounts
              config:
                issuer: https://accounts.google.com
                clientID: <client ID>
                clientSecret: <client secret>
                redirectURI: https://<public-cluster-dns-name>/dex/callback
                userIDKey: email
                userNameKey: email
#+end_src

And run =./konvoy up --yes= again to apply the change.

When the update is finished, Go to
=https://<public-cluster-dns-name>/token= and login with your Google
Account.

#+CAPTION: google-idp-token
[[file:images/google-idp-token.png]]

Follow the instructions in the page, but use the command below in the
second step to get the right value for the =server= parameter:

#+begin_src sh
  kubectl config set-cluster kubernetes-cluster \
      --certificate-authority=${HOME}/.kube/certs/kubernetes-cluster/k8s-ca.crt \
      --server=$(kubectl config view | grep server | awk '{ print $2 }')
#+end_src

Run the following command to check that you can administer the Kubernetes
cluster with your Google account:

#+begin_src sh
  kubectl get nodes
#+end_src

* 12. Upgrade a Konvoy cluster
  :PROPERTIES:
  :CUSTOM_ID: upgrade-a-konvoy-cluster
  :END:

Update the =~/.aws/credentials= file with the new information provided
by your instructor.

** Lab Steps
Edit the =cluster.yaml= file to change the Kubernetes version from
=1.15.1= to =1.15.2= in the 2 corresponding fields:

#+begin_src yaml
  ...
  spec:
    kubernetes:
      version: 1.15.2
  ...
    - name: worker
    addons:
      configVersion: stable-1.15.2-0
  ...
#+end_src

#+begin_src sh
  ./konvoy up --yes --upgrade --force-upgrade
#+end_src

#+begin_example

  This process will take about 15 minutes to complete (additional time may be required for larger clusters)

  STAGE [Provisioning Infrastructure]

  Initializing provider plugins...

  Terraform has been successfully initialized!
  Refreshing Terraform state in-memory prior to plan...
  The refreshed state will be used to calculate this plan, but will not be
  persisted to local or remote state storage.

  random_id.id: Refreshing state... (ID: jKY)

  ...

  No changes. Infrastructure is up-to-date.

  This means that Terraform did not detect any differences between your
  configuration and real physical resources that exist. As a result, no
  actions need to be performed.

  Apply complete! Resources: 0 added, 0 changed, 0 destroyed.

  Outputs:

  cluster_name = konvoy_v1.1.1-8ca6
  vpc_id = vpc-0941bb098eb24080d

  STAGE [Running Preflights]

  ...

  STAGE [Determining Upgrade Safety]

  ip-10-0-193-118.us-west-2.compute.internal                             [OK]
  ip-10-0-193-232.us-west-2.compute.internal                             [OK]
  ip-10-0-194-21.us-west-2.compute.internal                              [OK]
  ip-10-0-128-239.us-west-2.compute.internal                             [WARNING]
    - All replicas of the ReplicaSet "default/Nginx-7c45b84548" are running on this node.
  ip-10-0-128-64.us-west-2.compute.internal                              [WARNING]
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "plugins", which is unsafe for upgrades.
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "tmp", which is unsafe for upgrades.
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "plugin-dir", which is unsafe for upgrades.
    - Pod "default/jenkins-c79f457cb-vrjjq" is using EmptyDir volume "secrets-dir", which is unsafe for upgrades.
    - Pod "default/http-echo-2" is not being managed by a controller. Upgrading this node might result in data or availability loss.
    - Pod managed by ReplicaSet "default/jenkins-c79f457cb" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/jenkins-c79f457cb" are running on this node.
    - Pod managed by ReplicaSet "default/kudo-kafka-generator-d655d6dff" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/kudo-kafka-generator-d655d6dff" are running on this node.
  ip-10-0-129-247.us-west-2.compute.internal                             [WARNING]
    - Pod "default/http-echo-1" is not being managed by a controller. Upgrading this node might result in data or availability loss.
    - Pod managed by StatefulSet "kudo-system/kudo-controller-manager" is running on this node, and the StatefulSet does not have a replica count greater than 1.
  ip-10-0-129-41.us-west-2.compute.internal                              [OK]
  ip-10-0-129-88.us-west-2.compute.internal                              [WARNING]
    - Pod managed by ReplicaSet "default/ebs-dynamic-app-68b598758" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/ebs-dynamic-app-68b598758" are running on this node.
  ip-10-0-130-84.us-west-2.compute.internal                              [WARNING]
    - Pod managed by ReplicaSet "default/kudo-kafka-consumer-6b4dd5cd59" is running on this node, and the ReplicaSet does not have a replica count greater than 1.
    - All replicas of the ReplicaSet "default/kudo-kafka-consumer-6b4dd5cd59" are running on this node.
    - Pod "default/Redis" is not being managed by a controller. Upgrading this node might result in data or availability loss.

  STAGE [Upgrading Kubernetes]

  ...

  PLAY [Upgrade Nodes] ********************************************************************************************************************************************************************

  ...

  TASK [kubeadm-upgrade-nodes : drain node] ***********************************************************************************************************************************************
  changed: [10.0.129.184 -> ec2-54-191-70-155.us-west-2.compute.amazonaws.com]

  ...

  STAGE [Deploying Enabled Addons]
  helm                                                                   [OK]
  dashboard                                                              [OK]
  awsebscsiprovisioner                                                   [OK]
  opsportal                                                              [OK]
  fluentbit                                                              [OK]
  traefik                                                                [OK]
  kommander                                                              [OK]
  elasticsearch                                                          [OK]
  prometheus                                                             [OK]
  traefik-forward-auth                                                   [OK]
  dex                                                                    [OK]
  prometheusadapter                                                      [OK]
  kibana                                                                 [OK]
  elasticsearchexporter                                                  [OK]
  velero                                                                 [OK]
  dex-k8s-authenticator                                                  [OK]

  STAGE [Removing Disabled Addons]

  Kubernetes cluster and addons deployed successfully!

  Run `./konvoy apply kubeconfig` to update kubectl credentials.

  Navigate to the URL below to access various services running in the cluster.
    https://a1efd30f824244733adc1fb95157b9b1-2077667181.us-west-2.elb.amazonaws.com/ops/landing
  And login using the credentials below.
    Username: angry_williams
    Password: TNFGnFrZjhqaF0SNLoCzN3gvqrEsviTYxvMyuPv8KHU13ob6eNa0N7LfSVhd07Xk

  If the cluster was recently created, the dashboard and services may take a few minutes to be accessible.
#+end_example

If there is any error during the upgrade, run the
=./konvoy up --yes --upgrade --force-upgrade= again. It can happen when
the =drain= command times out.

Without the =--force-upgrade= flag, the Kubernetes nodes that have under
replicated pods wouldn't be upgraded.

Check the version of Kubernetes:

#+begin_src sh
  kubectl get nodes
#+end_src
pp#+begin_example
  NAME                                         STATUS   ROLES    AGE   VERSION
  ip-10-0-128-127.us-west-2.compute.internal   Ready    <none>   80m   v1.15.2
  ip-10-0-129-21.us-west-2.compute.internal    Ready    <none>   80m   v1.15.2
  ip-10-0-129-33.us-west-2.compute.internal    Ready    <none>   36m   v1.15.2
  ip-10-0-130-39.us-west-2.compute.internal    Ready    <none>   80m   v1.15.2
  ip-10-0-131-155.us-west-2.compute.internal   Ready    <none>   80m   v1.15.2
  ip-10-0-131-252.us-west-2.compute.internal   Ready    <none>   80m   v1.15.2
  ip-10-0-194-48.us-west-2.compute.internal    Ready    master   82m   v1.15.2
  ip-10-0-194-91.us-west-2.compute.internal    Ready    master   81m   v1.15.2
  ip-10-0-195-21.us-west-2.compute.internal    Ready    master   82m   v1.15.2
#+end_example

Check that the Redis and the http-echo apps are still accessible

#+begin_src sh
  telnet $(kubectl get svc Redis --output jsonpath={.status.loadBalancer.ingress[*].hostname}) 6379
#+end_src

#+begin_src sh
  curl -k -H "Host: http-echo-1.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
  curl -k -H "Host: http-echo-2.com" https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath={.status.loadBalancer.ingress[*].hostname})
#+end_src

* 13. Bonus Lab: Ingress troubleshooting
   :PROPERTIES:
   :CUSTOM_ID: ingress-troubleshooting
   :END:

In this section, we will leverage Konvoy logging to troubleshoot Ingress
failure issue.

We will deploy an Nginx application and expose it via L7 load balancer.
The application can be accessed with URLs follows below pattern.

=http[s]://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/Nginx/=

** Lab Steps
- first, let's deploy an Nginx application and scale it to 3 instances (replicas)

#+begin_src sh
  kubectl run --image=Nginx --replicas=3 --port=80 --restart=Always Nginx
#+end_src

- 2nd, expose a in cluster service

#+begin_src sh
  kubectl expose deploy Nginx --port 8080 --target-port 80 --type NodePort --name "svc-Nginx"
#+end_src

- 3rd, create ingress to expose service via Layer7 load balancer

#+begin_src sh
  cat << EOF | kubectl apply -f -
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: Nginx-root
    namespace: default
  spec:
    rules:
    - http:
        paths:
        - backend:
            serviceName: svc-Nginx
            servicePort: 8080
          path:  /applications/Nginx/
  EOF
#+end_src

- 4th, Now check Ingress configure in Traefik

#+CAPTION: Traefik Nginx
[[file:images/trafik_Nginx.png]]

The =Traefik dashboard= indicates the Nginx application is ready to
receive traffic but if you try access Nginx with URL listed below, you
will notice =404 Not Found= error like:

#+begin_src sh
  curl -k https://$(kubectl get svc traefik-kubeaddons -n kubeaddons --output jsonpath="{.status.loadBalancer.ingress[*].hostname}")/applications/Nginx/
#+end_src


#+CAPTION: Traefik Nginx
[[file:images/trafik_404.png]]

Let's troubleshoot this failure with Konvoy Kibana.

#+CAPTION: Kibana Nginx
[[file:images/kibana_Nginx.png]]

With Konvoy Kibana's near real time log collection and indexing, we can
easily identify that the ingress traffic was eventually handled by a pod
=kubernetes.pod_name:Nginx-755464dd6c-dnvp9= in Nginx service. The log
also gave us more information on the failure,
="GET /applications/Nginx/ HTTP/1.1" 404=, which tell us that Nginx
can't find resource at path =/applications/Nginx/=.

That is neat! Because w/o Kibana, you wouldn't know which pod in our Nginx
service handled this request. (Our Nginx deployment example launched 3 pods to
serve HTTP requests).  This could be even more complex if there are multiple
Nginx services existing on the same K8s cluster but hosted at different
namespaces.

To fix this failure requires some knowledge of Nginx configuration. In
general, when Nginx is launched with default configuration, it serves a
virtual directory on its =ROOT= path =(/)=. When it receives HTTP requests,
Nginx walks through its virtual directory to return resources to
the client.

In terms of our example, the =Ingress= configuration we submitted to k8s
was configured to a path at =/applications/Nginx/=. The =traefik=
ingress controller sees this =Ingress configuration= and forwards any
resource requests at path =/applications/Nginx/= to the downstream Nginx
service at the same path. The pod
=kubernetes.pod_name:Nginx-755464dd6c-dnvp9= received this request but the
Nginx instance in this pod failed to locate any resource under path
=/applications/Nginx/=. That is the reason we saw this failure,
="GET /applications/Nginx/ HTTP/1.1" 404=.

You can, of course, configure Nginx instance to serve resources at path
=/applications/Nginx/=. But an alternative solution is to leverage
=traefik= to strip PATH =/applications/Nginx/= to =ROOT (/)= before
routing requests to Nginx.

According to the =Traefik= documentation for [[https://docs.traefik.io/configuration/backends/kubernetes/][PathPrefixStrip]],
the annotation =(traefik.ingress.kubernetes.io/rule-type)= is exactly
what we need to direct traefik to strip ingress HOST PATH to ROOT PATH
aforementioned.

To update =Ingress=, we can use the command below.

#+begin_src sh
  cat << EOF | kubectl apply -f -
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      traefik.frontend.rule.type: PathPrefixStrip
    name: Nginx-root
    namespace: default
  spec:
    rules:
    - http:
        paths:
        - backend:
            serviceName: svc-Nginx
            servicePort: 8080
          path:  /applications/Nginx/
  EOF
#+end_src

#+CAPTION: dashboard Nginx
[[file:images/trafik_Nginx_200.png]]

#  LocalWords:  Jone noexport SETUPFILE num pri tex io src ps awscli sudo cp mv
# LocalWords:  aws mkdir sts UserId Arn keygen kubernetes chmod xvf kubeconfig zk
# LocalWords:  zsh addons init yaml availabilityZones YOUREMAIL nodePools svc pvc
# LocalWords:  configVersion apiVersion containerPort targetPort hashicorp http
# LocalWords:  NodePort backend serviceName servicePort kubeaddons NetworkPolicy
# LocalWords:  podSelector policyTypes matchLabels StorageClass creationTimestamp
# LocalWords:  awsebscsiprovisioner resourceVersion selfLink uid reclaimPolicy
# LocalWords:  volumeBindingMode WaitForFirstConsumer accessModes ReadWriteOnce
# LocalWords:  storageClassName Namespace Finalizers VolumeMode Filesystem awk
# LocalWords:  persistentvolume volumeMounts mountPath persistentVolumeClaim exe
# LocalWords:  claimName apache operatorVersion activePlan PlanExecution URIs idp
# LocalWords:  ClusterRoleBinding roleRef ClusterRole apiGroup clientSecret kube
# LocalWords:  redirectURI userIDKey userNameKey config vpc Preflights ReplicaSet
# LocalWords:  StatefulSet PathPrefixStrip namespace

* Appendix 1: Setting up an external identity provider
